import logging
import os
import subprocess
import re
import argparse

def get_gpu_info():
    """è·å–GPUä¿¡æ¯å¹¶è¿”å›ç›¸å…³å‚æ•°"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', 
                               '--format=csv,noheader,nounits'], 
                               capture_output=True, text=True, check=True)
        line = result.stdout.strip()
        if line:
            parts = line.split(', ')
            gpu_name = parts[0].strip()
            memory_mb = int(parts[1])
            compute_cap = float(parts[2])
            return gpu_name, memory_mb, compute_cap
    except:
        logging.warning("æ— æ³•è·å–GPUä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        return "Unknown GPU", 8192, 8.6
    
    return "Unknown GPU", 8192, 8.6

def get_gpu_optimized_config(gpu_name, memory_mb, compute_cap):
    """æ ¹æ®GPUå‹å·è¿”å›ä¼˜åŒ–é…ç½®"""
    config = {
        'workspace_size': 1 << 33,  # é»˜è®¤8GB
        'fp16_enabled': True,
        'int8_enabled': False,
        'fp8_enabled': False,  # æ–°å¢FP8æ”¯æŒ
        'dla_core': None,
        'gpu_suffix': 'unknown',
        'best_precision': 'fp16'  # é»˜è®¤æ¨èç²¾åº¦
    }
    
    # æ£€æŸ¥FP8ç¡¬ä»¶æ”¯æŒ - éœ€è¦è®¡ç®—èƒ½åŠ›8.9+ï¼ˆAda Lovelace/Hopperæ¶æ„ï¼‰
    fp8_hardware_support = compute_cap >= 8.9
    
    # æ£€æŸ¥TensorRT FP8è½¯ä»¶æ”¯æŒ
    fp8_software_support = False
    try:
        import tensorrt as trt
        version = trt.__version__
        version_parts = version.split('.')
        major_ver = int(version_parts[0])
        minor_ver = int(version_parts[1])
        
        # TensorRT 8.6+ æ”¯æŒFP8
        version_ok = major_ver > 8 or (major_ver == 8 and minor_ver >= 6)
        fp8_flag_available = hasattr(trt.BuilderFlag, 'FP8')
        fp8_software_support = version_ok and fp8_flag_available
        
        logging.info(f"TensorRTç‰ˆæœ¬: {version}, FP8è½¯ä»¶æ”¯æŒ: {fp8_software_support}")
    except ImportError:
        logging.warning("TensorRTæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥FP8æ”¯æŒ")
    
    # ç»¼åˆåˆ¤æ–­FP8å¯ç”¨æ€§
    config['fp8_enabled'] = fp8_hardware_support and fp8_software_support
    
    # RTX 30ç³»åˆ—ä¼˜åŒ–
    if 'RTX 3060' in gpu_name:
        config.update({
            'workspace_size': 1 << 32,  # 4GB (RTX 3060æ˜¾å­˜è¾ƒå°)
            'gpu_suffix': '3060',
            'best_precision': 'fp16'
        })
    elif 'RTX 3070' in gpu_name:
        config.update({
            'workspace_size': 1 << 33,  # 8GB
            'gpu_suffix': '3070',
            'best_precision': 'fp16'
        })
    elif 'RTX 3080' in gpu_name:
        config.update({
            'workspace_size': 1 << 34,  # 16GB
            'gpu_suffix': '3080',
            'best_precision': 'fp16'
        })
    elif 'RTX 3090' in gpu_name:
        config.update({
            'workspace_size': 1 << 34,  # 16GB
            'gpu_suffix': '3090',
            'best_precision': 'fp16'
        })
    
    # RTX 40ç³»åˆ—ä¼˜åŒ– - æ”¯æŒFP8
    elif 'RTX 4060' in gpu_name:
        config.update({
            'workspace_size': 1 << 32,  # 4GB
            'gpu_suffix': '4060',
            'best_precision': 'fp8' if config['fp8_enabled'] else 'fp16'
        })
    elif 'RTX 4070' in gpu_name:
        config.update({
            'workspace_size': 1 << 33,  # 8GB
            'gpu_suffix': '4070',
            'best_precision': 'fp8' if config['fp8_enabled'] else 'fp16'
        })
    elif 'RTX 4080' in gpu_name:
        config.update({
            'workspace_size': 1 << 34,  # 16GB
            'gpu_suffix': '4080',
            'best_precision': 'fp8' if config['fp8_enabled'] else 'fp16'
        })
    elif 'RTX 4090' in gpu_name:
        config.update({
            'workspace_size': 1 << 35,  # 32GB
            'gpu_suffix': '4090',
            'best_precision': 'fp8' if config['fp8_enabled'] else 'fp16'
        })
    
    # Tesla/ä¸“ä¸šå¡ä¼˜åŒ– - H100ç­‰æ”¯æŒFP8
    elif 'Tesla' in gpu_name or 'A100' in gpu_name or 'V100' in gpu_name or 'H100' in gpu_name or 'A800' in gpu_name:
        config.update({
            'workspace_size': 1 << 35,  # 32GB
            'int8_enabled': True,
            'gpu_suffix': 'tesla'
        })
        # H100/A800ç­‰é«˜ç«¯å¡ä¼˜å…ˆä½¿ç”¨FP8
        if 'H100' in gpu_name or 'A800' in gpu_name:
            config['best_precision'] = 'fp8' if config['fp8_enabled'] else 'fp16'
        else:
            config['best_precision'] = 'fp16'
    
    # æ ¹æ®æ˜¾å­˜è°ƒæ•´workspace - ä¿®å¤RTX 3070çš„é—®é¢˜
    if memory_mb < 6144:  # <6GB
        config['workspace_size'] = min(config['workspace_size'], 1 << 31)  # æœ€å¤§2GB
    elif memory_mb < 8192:  # <8GB ä½†ä¸åŒ…æ‹¬8192MBï¼ˆRTX 3070æ­£å¥½8GBï¼‰
        config['workspace_size'] = min(config['workspace_size'], 1 << 32)  # æœ€å¤§4GB
    elif memory_mb < 16384:  # <16GB
        config['workspace_size'] = min(config['workspace_size'], 1 << 33)  # æœ€å¤§8GB
    
    logging.info(f"GPU: {gpu_name}, æ˜¾å­˜: {memory_mb}MB, è®¡ç®—èƒ½åŠ›: {compute_cap}")
    logging.info(f"FP8ç¡¬ä»¶æ”¯æŒ: {fp8_hardware_support}, FP8è½¯ä»¶æ”¯æŒ: {fp8_software_support}")
    logging.info(f"æ¨èç²¾åº¦: {config['best_precision'].upper()}")
    logging.info(f"ä¼˜åŒ–é…ç½®: workspace={config['workspace_size']//1024//1024//1024}GB, FP16={config['fp16_enabled']}")
    
    return config

def convert_onnx_to_trt(trt_model, trt_kwargs, onnx_model, precision='auto', gpu_config=None):
    import tensorrt as trt
    logging.info("Converting onnx to trt...")
    
    # è·å–GPUé…ç½®
    if gpu_config is None:
        gpu_name, memory_mb, compute_cap = get_gpu_info()
        gpu_config = get_gpu_optimized_config(gpu_name, memory_mb, compute_cap)
    else:
        gpu_name = gpu_config.get('gpu_name', 'Unknown GPU')
    
    # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
    if precision == 'auto':
        precision = gpu_config['best_precision']
        logging.info(f"è‡ªåŠ¨é€‰æ‹©ç²¾åº¦: {precision.upper()}")
    
    # éªŒè¯ç²¾åº¦æ”¯æŒ
    if precision == 'fp8' and not gpu_config['fp8_enabled']:
        logging.warning(f"GPUä¸æ”¯æŒFP8ï¼Œå›é€€åˆ°FP16")
        precision = 'fp16'
    elif precision == 'int8' and not gpu_config['int8_enabled']:
        logging.warning(f"GPUé…ç½®ä¸æ¨èINT8ï¼Œå›é€€åˆ°FP16")
        precision = 'fp16'
    
    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    logger = trt.Logger(trt.Logger.INFO)
    builder = trt.Builder(logger)
    network = builder.create_network(network_flags)
    parser = trt.OnnxParser(network, logger)
    config = builder.create_builder_config()
    
    # ä½¿ç”¨GPUä¼˜åŒ–çš„é…ç½®
    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, gpu_config['workspace_size'])
    
    # RTX 3070ç‰¹åˆ«ä¼˜åŒ–
    if 'RTX 3070' in gpu_config.get('gpu_name', ''):
        # å¯ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–ç­–ç•¥
        config.set_flag(trt.BuilderFlag.STRICT_TYPES)  # ä¸¥æ ¼ç±»å‹è½¬æ¢
        config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)  # åå¥½ç²¾åº¦çº¦æŸ
        # é’ˆå¯¹RTX 3070çš„SM86æ¶æ„ä¼˜åŒ–
        config.default_device_type = trt.DeviceType.GPU
        logging.info("å¯ç”¨RTX 3070ç‰¹å®šä¼˜åŒ–")
    
    # è®¾ç½®ç²¾åº¦æ ‡å¿—
    if precision == 'fp16' and gpu_config['fp16_enabled']:
        config.set_flag(trt.BuilderFlag.FP16)
        logging.info("å¯ç”¨FP16ä¼˜åŒ–")
    elif precision == 'fp8' and gpu_config['fp8_enabled']:
        config.set_flag(trt.BuilderFlag.FP8)
        logging.info("å¯ç”¨FP8ä¼˜åŒ–")
    elif precision == 'int8' and gpu_config['int8_enabled']:
        config.set_flag(trt.BuilderFlag.INT8)
        logging.info("å¯ç”¨INT8ä¼˜åŒ–")
    elif precision == 'fp32':
        logging.info("ä½¿ç”¨FP32ç²¾åº¦")
    else:
        logging.warning(f"ä¸æ”¯æŒçš„ç²¾åº¦ {precision}ï¼Œä½¿ç”¨FP32")
        precision = 'fp32'
    
    # å¯ç”¨æ›´å¤šä¼˜åŒ–é€‰é¡¹
    config.set_flag(trt.BuilderFlag.DISABLE_TIMING_CACHE)  # ç¦ç”¨timing cacheä»¥è·å¾—æœ€æ–°ä¼˜åŒ–
    
    profile = builder.create_optimization_profile()
    # load onnx model
    with open(onnx_model, "rb") as f:
        if not parser.parse(f.read()):
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            raise ValueError('failed to parse {}'.format(onnx_model))
    
    # Check if engine_bytes is None before proceeding
    if len(trt_kwargs['input_names']) != len(trt_kwargs['min_shape']):
        raise ValueError(f"Mismatch: {len(trt_kwargs['input_names'])} input names vs {len(trt_kwargs['min_shape'])} shapes")
    
    # set input shapes
    for i in range(len(trt_kwargs['input_names'])):
        profile.set_shape(trt_kwargs['input_names'][i], trt_kwargs['min_shape'][i], trt_kwargs['opt_shape'][i], trt_kwargs['max_shape'][i])
    
    # æ ¹æ®ç²¾åº¦è®¾ç½®æ•°æ®ç±»å‹
    if precision == 'fp8':
        try:
            tensor_dtype = trt.DataType.FP8
        except AttributeError:
            logging.warning("å½“å‰TensorRTç‰ˆæœ¬ä¸æ”¯æŒFP8æ•°æ®ç±»å‹ï¼Œå›é€€åˆ°FP16")
            tensor_dtype = trt.DataType.HALF
    elif precision == 'fp16':
        tensor_dtype = trt.DataType.HALF
    elif precision == 'int8':
        tensor_dtype = trt.DataType.INT8
    else:  # fp32
        tensor_dtype = trt.DataType.FLOAT
    
    # set input and output data type
    for i in range(network.num_inputs):
        input_tensor = network.get_input(i)
        input_tensor.dtype = tensor_dtype
    for i in range(network.num_outputs):
        output_tensor = network.get_output(i)
        output_tensor.dtype = tensor_dtype
    config.add_optimization_profile(profile)
    
    # æ·»åŠ GPUé…ç½®åˆ°è¿”å›å€¼ä¸­ä»¥ä¾¿è°ƒè¯•
    gpu_config['gpu_name'] = gpu_name if 'gpu_name' not in gpu_config else gpu_config['gpu_name']
    
    engine_bytes = builder.build_serialized_network(network, config)
    
    # Check if build was successful
    if engine_bytes is None:
        raise RuntimeError("Failed to build TensorRT engine. Check the input configuration and ONNX model compatibility.")
    
    # save trt engine
    with open(trt_model, "wb") as f:
        f.write(engine_bytes)
    logging.info("Successfully convert onnx to trt...")
    
    return gpu_config

def get_trt_kwargs_cache_enabled():
    """Configuration for cache-enabled ONNX model (CosyVoice2)"""
    # Based on actual ONNX model inspection:
    # Input 0: x, Shape: [2, 80, 'seq_len']
    # Input 1: mask, Shape: [2, 1, 'seq_len']  
    # Input 2: mu, Shape: [2, 80, 'seq_len']
    # Input 3: t, Shape: [2]
    # Input 4: spks, Shape: [2, 80]
    # Input 5: cond, Shape: [2, 80, 'seq_len']
    # Input 6: down_blocks_conv_cache, Shape: [1, 2, 832, 2]
    # Input 7: down_blocks_kv_cache, Shape: [1, 4, 2, 'cache_in_len', 512, 2]
    # Input 8: mid_blocks_conv_cache, Shape: [12, 2, 512, 2]
    # Input 9: mid_blocks_kv_cache, Shape: [12, 4, 2, 'cache_in_len', 512, 2]
    # Input 10: up_blocks_conv_cache, Shape: [1, 2, 1024, 2]
    # Input 11: up_blocks_kv_cache, Shape: [1, 4, 2, 'cache_in_len', 512, 2]
    # Input 12: final_blocks_conv_cache, Shape: [2, 256, 2]
    
    min_shape = [
        (2, 80, 2),           # x
        (2, 1, 2),            # mask
        (2, 80, 2),           # mu  
        (2,),                 # t
        (2, 80),              # spks
        (2, 80, 2),           # cond
        (1, 2, 832, 2),       # down_blocks_conv_cache (static)
        (1, 4, 2, 2, 512, 2), # down_blocks_kv_cache
        (12, 2, 512, 2),      # mid_blocks_conv_cache (static)
        (12, 4, 2, 2, 512, 2), # mid_blocks_kv_cache
        (1, 2, 1024, 2),      # up_blocks_conv_cache (static)
        (1, 4, 2, 2, 512, 2), # up_blocks_kv_cache
        (2, 256, 2),          # final_blocks_conv_cache (static)
    ]
    
    opt_shape = [
        (2, 80, 200),           # x
        (2, 1, 200),            # mask
        (2, 80, 200),           # mu
        (2,),                   # t
        (2, 80),                # spks
        (2, 80, 200),           # cond
        (1, 2, 832, 2),         # down_blocks_conv_cache (static)
        (1, 4, 2, 50, 512, 2), # down_blocks_kv_cache
        (12, 2, 512, 2),        # mid_blocks_conv_cache (static)
        (12, 4, 2, 50, 512, 2), # mid_blocks_kv_cache
        (1, 2, 1024, 2),        # up_blocks_conv_cache (static)
        (1, 4, 2, 50, 512, 2), # up_blocks_kv_cache
        (2, 256, 2),            # final_blocks_conv_cache (static)
    ]
    
    max_shape = [
        (2, 80, 1000),           # x
        (2, 1, 1000),            # mask
        (2, 80, 1000),           # mu
        (2,),                    # t
        (2, 80),                 # spks
        (2, 80, 1000),           # cond
        (1, 2, 832, 2),          # down_blocks_conv_cache (static)
        (1, 4, 2, 50, 512, 2), # down_blocks_kv_cache
        (12, 2, 512, 2),         # mid_blocks_conv_cache (static)
        (12, 4, 2, 50, 512, 2), # mid_blocks_kv_cache
        (1, 2, 1024, 2),         # up_blocks_conv_cache (static)
        (1, 4, 2, 50, 512, 2), # up_blocks_kv_cache
        (2, 256, 2),             # final_blocks_conv_cache (static)
    ]
    
    input_names = [
        "x", "mask", "mu", "t", "spks", "cond", 
        "down_blocks_conv_cache", "down_blocks_kv_cache", 
        "mid_blocks_conv_cache", "mid_blocks_kv_cache",
        "up_blocks_conv_cache", "up_blocks_kv_cache", 
        "final_blocks_conv_cache"
    ]
    
    return {'min_shape': min_shape, 'opt_shape': opt_shape, 'max_shape': max_shape, 'input_names': input_names}

def get_trt_kwargs():
    """Configuration for standard ONNX model (CosyVoice)"""
    min_shape = [(2, 80, 4), (2, 1, 4), (2, 80, 4), (2, 80, 4)]
    opt_shape = [(2, 80, 200), (2, 1, 200), (2, 80, 200), (2, 80, 200)]
    max_shape = [(2, 80, 4000), (2, 1, 4000), (2, 80, 4000), (2, 80, 4000)]
    input_names = ["x", "mask", "mu", "cond"]
    return {'min_shape': min_shape, 'opt_shape': opt_shape, 'max_shape': max_shape, 'input_names': input_names}

if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    parser = argparse.ArgumentParser(description='ONNX to TensorRT conversion with automatic precision selection')
    parser.add_argument('--precision', choices=['auto', 'fp32', 'fp16', 'fp8', 'int8'], 
                       default='auto', help='ç›®æ ‡ç²¾åº¦ (é»˜è®¤: auto - è‡ªåŠ¨é€‰æ‹©æœ€ä½³)')
    parser.add_argument('--check-gpu', action='store_true', help='ä»…æ£€æŸ¥GPUä¿¡æ¯å¹¶æ˜¾ç¤ºæ¨èç²¾åº¦')
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    # è·å–GPUä¿¡æ¯
    gpu_name, memory_mb, compute_cap = get_gpu_info()
    gpu_config = get_gpu_optimized_config(gpu_name, memory_mb, compute_cap)
    
    # å¦‚æœåªæ˜¯æ£€æŸ¥GPUä¿¡æ¯
    if args.check_gpu:
        print("\n" + "="*70)
        print("ğŸ® GPU ä¿¡æ¯ä¸ç²¾åº¦æ¨è")
        print("="*70)
        print(f"GPUå‹å·: {gpu_name}")
        print(f"æ˜¾å­˜å¤§å°: {memory_mb} MB ({memory_mb/1024:.1f} GB)")
        print(f"è®¡ç®—èƒ½åŠ›: {compute_cap}")
        print(f"FP8ç¡¬ä»¶æ”¯æŒ: {'âœ… æ”¯æŒ' if compute_cap >= 8.9 else 'âŒ ä¸æ”¯æŒ (éœ€è¦8.9+)'}")
        print(f"FP8è½¯ä»¶æ”¯æŒ: {'âœ… æ”¯æŒ' if gpu_config['fp8_enabled'] else 'âŒ ä¸æ”¯æŒ'}")
        print(f"æ¨èç²¾åº¦: {gpu_config['best_precision'].upper()} â­")
        
        print(f"\nğŸ’¡ ç²¾åº¦è¯´æ˜:")
        print(f"  â€¢ FP8: æœ€å°æ¨¡å‹ä½“ç§¯ï¼Œéœ€è¦RTX 40ç³»åˆ—/H100ç­‰")
        print(f"  â€¢ FP16: å¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦ï¼Œæ¨èé€‰æ‹©")
        print(f"  â€¢ FP32: æœ€é«˜ç²¾åº¦ï¼Œæ¨¡å‹ä½“ç§¯è¾ƒå¤§")
        print(f"  â€¢ INT8: éœ€è¦æ ¡å‡†æ•°æ®é›†")
        print("="*70)
        exit(0)
    
    model_dir = "/home/yangzy/nas/pretrained_models/ca_cosyvoice"
    onnx_model = os.path.join(model_dir, "flow.decoder.estimator.fp32.onnx")
    
    # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„ç²¾åº¦
    if args.precision == 'auto':
        final_precision = gpu_config['best_precision']
        logging.info(f"ğŸ¤– è‡ªåŠ¨é€‰æ‹©ç²¾åº¦: {final_precision.upper()}")
    else:
        final_precision = args.precision
        logging.info(f"ğŸ¯ æ‰‹åŠ¨æŒ‡å®šç²¾åº¦: {final_precision.upper()}")
    
    # éªŒè¯ç²¾åº¦å¯ç”¨æ€§
    if final_precision == 'fp8' and not gpu_config['fp8_enabled']:
        logging.error("âŒ å½“å‰ç¯å¢ƒä¸æ”¯æŒFP8è½¬æ¢")
        if compute_cap < 8.9:
            logging.info("ğŸ’¡ æ‚¨çš„GPUè®¡ç®—èƒ½åŠ›ä¸º %.1fï¼Œéœ€è¦ 8.9+ æ‰æ”¯æŒFP8", compute_cap)
        else:
            logging.info("ğŸ’¡ è¯·æ£€æŸ¥TensorRTç‰ˆæœ¬æ˜¯å¦ >= 8.6")
        logging.info("ğŸ”„ è‡ªåŠ¨å›é€€åˆ°FP16ç²¾åº¦")
        final_precision = 'fp16'
    
    # æ ¹æ®æœ€ç»ˆç²¾åº¦ç”ŸæˆTRTæ–‡ä»¶å
    trt_filename = f"flow.decoder.estimator.{final_precision}.{gpu_config['gpu_suffix']}.plan"
    trt_model = os.path.join(model_dir, trt_filename)
    
    logging.info(f"ğŸ“ è¾“å…¥ONNXæ¨¡å‹: {onnx_model}")
    logging.info(f"ğŸ“ è¾“å‡ºTRTæ¨¡å‹: {trt_model}")
    logging.info(f"ğŸ¯ ä½¿ç”¨ç²¾åº¦: {final_precision.upper()}")
    
    # ä½¿ç”¨cache-enabledé…ç½®è¿›è¡Œè½¬æ¢
    try:
        final_config = convert_onnx_to_trt(trt_model, get_trt_kwargs_cache_enabled(), onnx_model, final_precision, gpu_config)
        
        logging.info(f"âœ… TensorRTè½¬æ¢å®Œæˆï¼")
        logging.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: {trt_model}")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°å’Œæ€§èƒ½é¢„æœŸ
        if os.path.exists(trt_model):
            file_size = os.path.getsize(trt_model) / 1024 / 1024  # MB
            logging.info(f"ğŸ“Š TRTæ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
            
            # æ€§èƒ½é¢„æœŸè¯´æ˜
            if final_precision == 'fp8':
                logging.info("ğŸš€ FP8æ¨¡å‹: æœ€é«˜æ¨ç†é€Ÿåº¦ï¼Œæœ€å°æ˜¾å­˜å ç”¨")
            elif final_precision == 'fp16':
                logging.info("âš¡ FP16æ¨¡å‹: è‰¯å¥½çš„é€Ÿåº¦ä¸ç²¾åº¦å¹³è¡¡")
            elif final_precision == 'fp32':
                logging.info("ğŸ¯ FP32æ¨¡å‹: æœ€é«˜ç²¾åº¦ï¼Œè¾ƒå¤§æ˜¾å­˜å ç”¨")
                
    except Exception as e:
        logging.error(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        logging.info("ğŸ’¡ å»ºè®®:")
        logging.info("  1. æ£€æŸ¥ONNXæ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ")
        logging.info("  2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ˜¾å­˜ç©ºé—´")
        logging.info("  3. å°è¯•é™ä½ç²¾åº¦æˆ–å‡å°batch size")
        raise