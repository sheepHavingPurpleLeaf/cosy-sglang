#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import numpy as np
import logging
from typing import List, Dict, Optional, Generator
from pathlib import Path
import threading
from contextlib import nullcontext
import os
import sys
sys.path.append('third_party/Matcha-TTS')
# æ·»åŠ yamlé…ç½®æ–‡ä»¶åŠ è½½
from hyperpyyaml import load_hyperpyyaml
from cosyvoice.utils.common import fade_in_out
from cosyvoice.utils.common import TrtContextWrapper
# å¯¼å…¥æ‰€éœ€æ¨¡å‹ç±»
import tensorrt as trt
# å¯¼å…¥è¯·æ±‚åº“
import requests
import json
import time
import pdb
import torchaudio
import torch.nn.functional as F
from cosyvoice.utils.mask import make_pad_mask
from collections import deque
import queue

class CosyVoiceCA:
    """
    ä¸€ä¸ªç®€åŒ–ç‰ˆçš„CosyVoice2ç±»ï¼Œä»…åŠ è½½Flowå’ŒHiFiGANæ¨¡å‹ï¼Œ
    åªæ‰§è¡Œtoken2wavçš„æµå¼é€»è¾‘ï¼Œä¸æ‰§è¡ŒLLMéƒ¨åˆ†
    """
    
    def __init__(self, model_dir):
        """
        åˆå§‹åŒ–CosyVoiceCAç±»ï¼Œä½¿ç”¨å›ºå®šé…ç½®ï¼š
        load_jit=False, load_trt=True, fp16=True, use_flow_cache=True
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•ï¼ŒåŒ…å«flowå’Œhiftæ¨¡å‹
            device: æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ï¼Œé»˜è®¤ä¸ºNoneï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self.device = torch.device('cuda')
        self.model_dir = Path(model_dir)
        
        # å›ºå®šé…ç½®
        self.fp16 = True
        self.is_synthesizing = False
        
        # åŠ è½½yamlé…ç½®æ–‡ä»¶ï¼Œä¸åŸç‰ˆæ–¹å¼ä¸€è‡´
        hyper_yaml_path = os.path.join(self.model_dir, "cosyvoice2ca.yaml")
        if not os.path.exists(hyper_yaml_path):
            raise ValueError(f"{hyper_yaml_path} é…ç½®æ–‡ä»¶ä¸å­˜åœ¨!")
        
        with open(hyper_yaml_path, 'r') as f:
            self.configs = load_hyperpyyaml(f, overrides={'qwen_pretrain_path': os.path.join(self.model_dir, 'CosyVoice-BlankEN')})
            self.sgl_config = self.configs.get('sgl_config', {})
            self.base_url = self.sgl_config.get('base_url', 'http://localhost:30000')
            
        # ä½¿ç”¨dequeæ›¿ä»£listï¼Œæé«˜æ€§èƒ½
        self.speech_tokens = deque()
        self.offset = 151936

        self.token_hop_len = 25
        self.token_overlap_len = 20
        self.llm_end = False
        # ä»é…ç½®è·å–é‡‡æ ·ç‡å’Œå…¶ä»–å‚æ•°
        self.sample_rate = self.configs.get('sample_rate', 24000)
        self.token_mel_ratio = self.configs.get('token_mel_ratio', 2)
        self.token_frame_rate = self.configs.get('token_frame_rate', 25)
        
        # åˆå§‹åŒ–æ¨¡å‹
        logging.info(f"Loading models from {self.model_dir}...")
        self.first_chunk_buffer_len = 120
        self._init_models()
        
        # è®¾ç½®flowç¼“å­˜å¤§å°
        self.mel_cache_len = 8
        self.source_cache_len = int(self.mel_cache_len * 480)
        self.speech_window = np.hamming(2 * self.source_cache_len)
        
        # å¤šçº¿ç¨‹ç›¸å…³ - ä½¿ç”¨äº‹ä»¶æ›¿ä»£è½®è¯¢
        self.lock = threading.Lock()
        self.token_ready_event = threading.Event()
        self.token_queue = queue.Queue()
        
        # é¢„åˆ†é…å¸¸ç”¨çš„å¼ é‡
        self.zero_cache_source = torch.zeros(1, 1, 0, device=self.device)
        
        # é¢„åˆ†é…å¼ é‡ç¼“å†²åŒºï¼Œé¿å…é¢‘ç¹åˆ›å»º
        max_token_len = self.token_hop_len + self.flow.pre_lookahead_len
        self.token_buffer = torch.zeros(1, max_token_len, dtype=torch.int32, device=self.device)
        self.token_len_buffer = torch.zeros(1, dtype=torch.int32, device=self.device)
        
        # å¤ç”¨sessionä»¥å‡å°‘è¿æ¥å¼€é”€
        self.session = requests.Session()

        
        if os.path.exists(os.path.join(self.model_dir, "warmup_tokens.pt")):
            logging.info("warmup start")
            warmup_tokens = torch.load(os.path.join(self.model_dir, "warmup_tokens.pt"))
            if isinstance(warmup_tokens, list):
                warmup_tokens = torch.stack(warmup_tokens)
            warmup_pointer = 0
            hop_size = self.token_hop_len + self.prompt_token_pad + self.flow.pre_lookahead_len
            while warmup_pointer + hop_size < len(warmup_tokens):
                self.flow_inputs['token'] = warmup_tokens[warmup_pointer:warmup_pointer + hop_size].unsqueeze(0)
                self.flow_inputs['token_offset'] = 0
                self.flow_inputs['mask'] = self.spk2info['mask']
                _ = self.token2wav(**self.flow_inputs)
                warmup_pointer += self.token_hop_len
        else:
            logging.info("Warmup tokens not found, skipping warmup")
        # Convert list of tensors to a single tensor
        logging.info(f"CosyVoiceCAåˆå§‹åŒ–å®Œæˆ")
        
    def _init_models(self):
        self.flow = self.configs['flow']
        self.flow_decoder_required_cache_size = self.token_hop_len * self.flow.token_mel_ratio
        self.hift = self.configs['hift']
        self.hift_cache = None
        self.tokenizer = self.configs['get_tokenizer']()
        
        # Add allowed_special attribute for tokenizer
        self.allowed_special = self.configs['allowed_special']

        self.token_min_hop_len = 2 * self.flow.input_frame_rate
        self.token_max_hop_len = 4 * self.flow.input_frame_rate
        
        # åº”ç”¨FP16è®¾ç½®
        if self.fp16 and torch.cuda.is_available():
            self.flow.half()
            
        # åŠ è½½æ¨¡å‹å‚æ•°
        self._load_models()
            
    def _load_models(self):
        """æŒ‰ç…§åŸç‰ˆåŠ è½½æ¨¡å‹æƒé‡"""
        try:
            # ç¡®å®šåŠ è½½çš„æ¨¡å‹è·¯å¾„
            flow_path = os.path.join(self.model_dir, "flow.cache.pt")
                
            hift_path = os.path.join(self.model_dir, "hift.pt")
            # spk2info = os.path.join(self.model_dir, "spk2info-old-newcreate.pt")
            spk2info = os.path.join(self.model_dir, "spk2info-normal.pt")
            # spk2info = os.path.join(self.model_dir, "spk2info-tong.pt")
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(flow_path):
                raise ValueError(f"Flowæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {flow_path}")
            if not os.path.exists(hift_path):
                raise ValueError(f"HiFTæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {hift_path}")
            if not os.path.exists(spk2info):
                raise ValueError(f"spk2infoæ–‡ä»¶ä¸å­˜åœ¨: {spk2info}")
            
            self.spk2info = torch.load(spk2info, map_location=self.device)['my_zero_shot_spk']
            
            # åŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸åŸç‰ˆCosyVoice2ä¸€è‡´
            self.flow.load_state_dict(torch.load(flow_path, map_location=self.device), strict=True)
            self.flow.to(self.device).eval()
            # flow_encoder = torch.jit.load(os.path.join(self.model_dir, "flow.encoder.fp16.zip"), map_location=self.device)
            # self.flow.encoder = flow_encoder
            flow_embedding = F.normalize(self.spk2info['flow_embedding'].half().to(self.device), dim=1)
            self.spk2info['flow_embedding'] = self.flow.spk_embed_affine_layer(flow_embedding).to(self.device)
            self.prompt_token_pad = int(np.ceil(self.spk2info['flow_prompt_speech_token'].size(1) / self.token_hop_len) * self.token_hop_len - self.spk2info['flow_prompt_speech_token'].size(1))
    
            self.spk2info['mask'] = (~make_pad_mask(torch.tensor([self.flow.pre_lookahead_len + self.token_hop_len + self.prompt_token_pad + self.spk2info['flow_prompt_speech_token'].size(1)]))).unsqueeze(-1).to(self.device)
            self.flow_inputs = {
                'prompt_token': self.spk2info['flow_prompt_speech_token'].to(self.device),
                'prompt_token_len': torch.tensor([self.spk2info['flow_prompt_speech_token'].size(1)], dtype=torch.int32).to(self.device),
                'prompt_feat': self.spk2info['prompt_speech_feat'].to(self.device),
                'prompt_feat_len': torch.tensor([self.spk2info['prompt_speech_feat'].size(1)], dtype=torch.int32).to(self.device),
                'embedding': self.spk2info['flow_embedding'],
                'finalize': False,
                'mask': self.spk2info['mask']
            }  
            # å¤„ç†HiFTæ¨¡å‹ï¼ŒæŒ‰ç…§CosyVoice2å¤„ç†æ–¹å¼
            hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_path, map_location=self.device).items()}
            self.hift.load_state_dict(hift_state_dict, strict=True)
            self.hift.to(self.device).eval()
            
            self._load_trt_models()
            
            logging.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: token_mel_ratio={self.token_mel_ratio}, token_frame_rate={self.token_frame_rate}")
        except Exception as e:
            logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
            
    def _load_trt_models(self):
        """åŠ è½½TensorRTæ¨¡å‹ï¼Œå¦‚æœéœ€è¦"""
        try:
            # TensorRTå¼•æ“æ–‡ä»¶è·¯å¾„
            flow_decoder_estimator_model = os.path.join(
                self.model_dir, 
                # f"flow.decoder.estimator.fp16.mygpu.plan"
                f"flow.decoder.estimator.fp16.new.plan"
                # f"flow.decoder.estimator.fp16.A10.plan"
                # f"flow.decoder.estimator.fp16.3070.plan"
            )
            
            if os.path.getsize(flow_decoder_estimator_model) == 0:
                raise ValueError(f"{flow_decoder_estimator_model}æ˜¯ç©ºæ–‡ä»¶ï¼Œè¯·åˆ é™¤åé‡æ–°å¯¼å‡º!")
                
            # åŠ è½½TensorRTå¼•æ“
            del self.flow.decoder.estimator
            with open(flow_decoder_estimator_model, 'rb') as f:
                self.flow.decoder.estimator_engine = trt.Runtime(trt.Logger(trt.Logger.INFO)).deserialize_cuda_engine(f.read())
                
            assert self.flow.decoder.estimator_engine is not None, f"åŠ è½½TensorRTå¤±è´¥: {flow_decoder_estimator_model}"
            self.flow.decoder.estimator = TrtContextWrapper(self.flow.decoder.estimator_engine, trt_concurrent = 1, device=self.device)

            
            logging.info("TensorRTæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logging.error(f"TensorRTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def _send_stream_request(self, payload):
        """å‘é€æµå¼è¯·æ±‚å¹¶å¤„ç†å“åº”æµ - ä¼˜åŒ–ç‰ˆæœ¬"""

        processed_tokens = 0
        try:
            # å‘é€è¯·æ±‚ - å¤ç”¨session
            response = self.session.post(
                self.base_url + "/generate",
                json=payload,
                stream=True,
                timeout=60
            )
            
            if response.status_code != 200:
                logging.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                yield {"error": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"}
                return
            
            # å¤„ç†æµå¼å“åº” - å‡å°‘ä¸å¿…è¦çš„è®¡ç®—
            for chunk in response.iter_lines(decode_unicode=False):
                if not chunk:
                    continue
                chunk_str = chunk.decode("utf-8")
                
                if chunk_str == "data: [DONE]":
                    self.llm_end = True
                    break
                    
                if chunk_str.startswith("data:"):
                    try:
                        data = json.loads(chunk_str[5:].strip())
                        if "output_ids" in data:
                            current_tokens = data["output_ids"]
                            
                            # è®¡ç®—æ–°å¢çš„token
                            new_tokens = current_tokens[processed_tokens:]
                            processed_tokens = len(current_tokens)
                            
                            # æ·»åŠ åˆ°æ€»tokenåˆ—è¡¨
                            yield {"tokens": new_tokens, "is_partial": True}
                            
                    except json.JSONDecodeError as e:
                        logging.error(f"è§£æJSONæ—¶å‡ºé”™: {e} - åŸå§‹æ•°æ®: {chunk_str}")
            
                
        except requests.RequestException as e:
            logging.error(f"è¯·æ±‚é”™è¯¯: {str(e)}")
            yield {"error": f"è¯·æ±‚é”™è¯¯: {str(e)}"}
        except Exception as e:
            logging.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}")
            yield {"error": f"å¤„ç†å“åº”é”™è¯¯: {str(e)}"}
    
    def _prepare_input_features(self, text: str):
        """å‡†å¤‡è¾“å…¥ç‰¹å¾"""
        text_token = self.tokenizer.encode(text, allowed_special=self.allowed_special)
        # print(text_token, text)
        text_len = len(text_token)
        model_input = self.spk2info
        prompt_text = model_input['prompt_text'].squeeze().tolist()
        llm_input = [158500] + prompt_text +text_token + [158497]
        # text_token = torch.tensor([text_token], dtype=torch.int32).to(self.device)
        # text_token_len = torch.tensor([text_token.shape[1]], dtype=torch.int32).to(self.device)

        speech_prompt_token = model_input['llm_prompt_speech_token'] + self.offset
        llm_input = llm_input + speech_prompt_token.squeeze().tolist()
        model_input['llm_input'] = llm_input
        model_input['text_len'] = text_len
        return model_input


        # è·å–è¾“å…¥æ–‡æœ¬çš„token
    def _llm_job(self, input_ids: List[int], text_len: int):
        """
        å¤„ç†LLMä»»åŠ¡
        
        Args:
            input_ids: è¾“å…¥çš„token IDs
        """
        payload = {
            "input_ids": input_ids,
            "stream": True,
            "sampling_params": {
                "top_p": 0.8, 
                "top_k": 25, 
                "temperature": 1.0, 
                "max_new_tokens": text_len * self.sgl_config.get('max_token_text_ratio', 20), 
                "ras_penalty": 3.0,
                "stop_token_ids": [158497]
            }
        }
        
        for i in self._send_stream_request(payload):
            # ä½¿ç”¨é”ä¿æŠ¤å¯¹speech_tokensçš„ä¿®æ”¹
            with self.lock:
                self.speech_tokens.extend(i["tokens"])
                if self.speech_tokens and self.speech_tokens[-1] == 6561 + self.offset:
                    self.speech_tokens.pop()
            
            # if len(self.speech_tokens) >= self.token_hop_len + self.flow.pre_lookahead_len:
            self.token_ready_event.set()
        
        # LLMä»»åŠ¡ç»“æŸæ ‡å¿—
        with self.lock:
            self.llm_end = True
        self.token_ready_event.set()  # ç¡®ä¿ä¸»çº¿ç¨‹èƒ½è¢«å”¤é†’
    
    def _reset_cuda_state(self):
        """é‡ç½®CUDAçŠ¶æ€å’Œå®ä¾‹çŠ¶æ€ï¼Œç”¨äºé”™è¯¯æ¢å¤"""
        try:
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
            
            # é‡ç½®å®ä¾‹çŠ¶æ€
            with self.lock:
                self.speech_tokens.clear()  # ä½¿ç”¨clear()æ¥æ¸…ç©ºdeque
                self.llm_end = False
            self.is_synthesizing = False
            self.hift_cache = None
            
            # é‡ç½® flow_inputs çš„åŠ¨æ€éƒ¨åˆ†
            if hasattr(self, 'flow_inputs'):
                self.flow_inputs['finalize'] = False
                self.flow_inputs['mask'] = self.spk2info['mask']
                if 'token' in self.flow_inputs:
                    del self.flow_inputs['token']
                if 'token_offset' in self.flow_inputs:
                    del self.flow_inputs['token_offset']
            
            logging.info("CUDAçŠ¶æ€å’Œå®ä¾‹çŠ¶æ€å·²é‡ç½®")
            return True
        except Exception as e:
            logging.error(f"é‡ç½®çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False

    def _safe_token2wav(self, **kwargs):
        """å¸¦å®¹é”™çš„ token2wav æ–¹æ³•"""
        max_retries = 2
        
        for attempt in range(max_retries):
            try:
                return self.token2wav(**kwargs)
            except RuntimeError as e:
                error_msg = str(e)
                if "device-side assert triggered" in error_msg or "CUDA error" in error_msg:
                    logging.warning(f"CUDAé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                    
                    if attempt < max_retries - 1:
                        # å°è¯•æ¢å¤
                        logging.info("å°è¯•æ¢å¤CUDAçŠ¶æ€...")
                        self._reset_cuda_state()
                        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…
                        continue
                    else:
                        logging.error("CUDAé”™è¯¯æ¢å¤å¤±è´¥ï¼Œè·³è¿‡å½“å‰chunk")
                        return None
                else:
                    # éCUDAé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise
        
        return None

    def synthesize(self, text: str, session_id: str = "default") -> Generator[Dict, None, None]:
        """
        åˆæˆè¯­éŸ³ - å¸¦å®¹é”™æœºåˆ¶
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            session_id: ä¼šè¯ID
            
        Returns:
            ç”Ÿæˆå™¨ï¼Œäº§ç”ŸåŒ…å«ç”Ÿæˆçš„tokençš„å­—å…¸
        """
        if self.is_synthesizing is True:
            logging.info("æ­£åœ¨åˆæˆä¸­ï¼Œè¯·ç¨åå†è¯•")
            return
        
        # åˆå§‹çŠ¶æ€è®¾ç½®
        self.is_synthesizing = True
        synthesis_success = False
        
        try:
            # é‡ç½®çŠ¶æ€
            self.hift_cache = None
            
                                      # ä½¿ç”¨é”ä¿æŠ¤çŠ¶æ€åˆå§‹åŒ–
            with self.lock:
                self.speech_tokens.clear()
                self.llm_end = False
            
            model_input = self._prepare_input_features(text)
            time_llm_start = time.time()
            p = threading.Thread(target=self._llm_job, args=(model_input['llm_input'], model_input['text_len']))
            p.start()
            print("synthesize text:", text)
            is_first_chunk = True
            token_offset = 0
            chunks_generated = 0
            
            while True:
                # Wait for tokens to be ready
                self.token_ready_event.wait(timeout=0.1)
                
                # ä½¿ç”¨é”ä¿æŠ¤å¯¹speech_tokensçš„è®¿é—®å’ŒçŠ¶æ€æ£€æŸ¥
                with self.lock:
                    if not self.speech_tokens:
                        if self.llm_end:
                            break
                        self.token_ready_event.clear()
                        continue
                
                this_token_hop_len = self.token_hop_len + self.prompt_token_pad if token_offset == 0 else self.token_hop_len
                required_tokens = this_token_hop_len + self.flow.pre_lookahead_len
                
                if len(self.speech_tokens) - token_offset >= required_tokens:
                    time_llm_first_chunk_end = time.time()
                    print(f"llm first chunk time in ms: {(time_llm_first_chunk_end - time_llm_start) * 1000}")
                    # ä¿®å¤ï¼šå°† deque è½¬æ¢ä¸º list ä»¥æ”¯æŒåˆ‡ç‰‡æ“ä½œ
                    speech_tokens_list = list(self.speech_tokens)
                    this_tts_speech_token = torch.tensor(speech_tokens_list[:token_offset + required_tokens]).unsqueeze(dim=0) - self.offset
                    print(f"å½“å‰tokenæ•°: {len(self.speech_tokens)}, éœ€è¦: {required_tokens}, è·³è·ƒé•¿åº¦: {this_token_hop_len}")
                    
                    # å‡†å¤‡å‚æ•°
                    self.flow_inputs['token'] = this_tts_speech_token
                    self.flow_inputs['token_offset'] = token_offset
                    if is_first_chunk:
                        self.flow_inputs['mask'] = self.spk2info['mask']
                        is_first_chunk = False
                    else: 
                        self.flow_inputs['mask'] = None
                    
                    # å®‰å…¨è°ƒç”¨ token2wav
                    try:
                        this_tts_speech = self._safe_token2wav(**self.flow_inputs)
                        
                        if this_tts_speech is not None:
                            token_offset += this_token_hop_len
                            chunks_generated += 1
                            yield {'tts_speech': this_tts_speech.cpu()}
                        else:
                            # å¦‚æœå½“å‰chunkå¤±è´¥ï¼Œè®°å½•å¹¶ç»§ç»­
                            logging.warning(f"Chunk {chunks_generated + 1} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                            token_offset += this_token_hop_len
                            
                    except Exception as e:
                        logging.error(f"å¤„ç†éŸ³é¢‘chunkæ—¶å‡ºé”™: {e}")
                        # å°è¯•æ¢å¤å¹¶ç»§ç»­
                        self._reset_cuda_state()
                        token_offset += this_token_hop_len
                        continue
                
                if self.llm_end and len(self.speech_tokens) - token_offset < required_tokens:
                    break
            
            # ç­‰å¾…LLMçº¿ç¨‹ç»“æŸ
            p.join()
            
            # æœ€ç»ˆå¤„ç†
            try:
                if len(self.speech_tokens) > token_offset:
                    # ä¿®å¤ï¼šå°† deque è½¬æ¢ä¸º list ä»¥æ”¯æŒ torch.tensor è½¬æ¢
                    speech_tokens_list = list(self.speech_tokens)
                    this_tts_speech_token = torch.tensor(speech_tokens_list).unsqueeze(dim=0) - self.offset
                    self.flow_inputs['token'] = this_tts_speech_token
                    self.flow_inputs['token_offset'] = token_offset
                    self.flow_inputs['finalize'] = True
                    self.flow_inputs['mask'] = None
                    
                    final_speech = self._safe_token2wav(**self.flow_inputs)
                    if final_speech is not None:
                        yield {'tts_speech': final_speech.cpu()}
                        synthesis_success = True
                    else:
                        logging.warning("æœ€ç»ˆchunkç”Ÿæˆå¤±è´¥")
                
            except Exception as e:
                logging.error(f"æœ€ç»ˆå¤„ç†æ—¶å‡ºé”™: {e}")
                # å³ä½¿æœ€ç»ˆå¤„ç†å¤±è´¥ï¼Œå¦‚æœä¹‹å‰æœ‰æˆåŠŸçš„chunkï¼Œä¹Ÿç®—éƒ¨åˆ†æˆåŠŸ
                if chunks_generated > 0:
                    synthesis_success = True
        
        except Exception as e:
            logging.error(f"åˆæˆè¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
            import traceback
            logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        finally:
            # ç¡®ä¿çŠ¶æ€æ­£ç¡®é‡ç½®
            try:
                # ä½¿ç”¨é”ä¿æŠ¤çŠ¶æ€é‡ç½®
                with self.lock:
                    self.speech_tokens.clear()
                    self.llm_end = False
                    self.is_synthesizing = False
                    if hasattr(self, 'flow_inputs'):
                        self.flow_inputs['finalize'] = False
                
                # æ¸…ç†CUDAç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.current_stream().synchronize()
                
                if synthesis_success:
                    logging.info(f"æ–‡æœ¬åˆæˆå®Œæˆ: '{text[:50]}{'...' if len(text) > 50 else ''}'")
                else:
                    logging.warning(f"æ–‡æœ¬åˆæˆå¤±è´¥æˆ–éƒ¨åˆ†å¤±è´¥: '{text[:50]}{'...' if len(text) > 50 else ''}'")
                    
            except Exception as cleanup_error:
                logging.error(f"æ¸…ç†çŠ¶æ€æ—¶å‡ºé”™: {cleanup_error}")
                # å¼ºåˆ¶é‡ç½®å…³é”®çŠ¶æ€
                self.is_synthesizing = False

    def token2wav(self, token, prompt_token, prompt_token_len, prompt_feat, prompt_feat_len, embedding, token_offset, finalize=False, mask=None):
        with torch.cuda.amp.autocast(self.fp16):
            tts_mel, _ = self.flow.inference(token=token.to(self.device),
                                             token_len=torch.tensor([token.shape[1]], dtype=torch.int32).to(self.device),
                                             prompt_token=prompt_token.to(self.device),
                                             prompt_token_len=prompt_token_len,
                                             prompt_feat=prompt_feat,
                                             prompt_feat_len=prompt_feat_len,
                                             embedding=embedding,
                                             streaming=True,
                                             finalize=finalize,
                                             mask=mask)
            time_start = time.time()
            tts_mel = tts_mel[:, :, token_offset * self.flow.token_mel_ratio:]                # append hift cache
            if self.hift_cache is not None:
                hift_cache_mel, hift_cache_source = self.hift_cache['mel'], self.hift_cache['source']
                tts_mel = torch.concat([hift_cache_mel, tts_mel], dim=2)
            else:
                hift_cache_source = self.zero_cache_source  # ä½¿ç”¨é¢„åˆ†é…çš„é›¶å¼ é‡
            
            # keep overlap mel and hift cache
            if finalize is False:
                time_hift_inference_start = time.time()
                tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)
                time_hift_inference_end = time.time()
                print(f"hift inference time in ms: {(time_hift_inference_end - time_hift_inference_start) * 1000}")
                if self.hift_cache is not None:
                    tts_speech = fade_in_out(tts_speech, self.hift_cache['speech'], self.speech_window)
                
                self.hift_cache = {'mel': tts_mel[:, :, -self.mel_cache_len:],
                                            'source': tts_source[:, :, -self.source_cache_len:],
                                            'speech': tts_speech[:, -self.source_cache_len:]}
                tts_speech = tts_speech[:, :-self.source_cache_len]
            else:
                tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)
                if self.hift_cache is not None:
                    tts_speech = fade_in_out(tts_speech, self.hift_cache['speech'], self.speech_window)
            time_end = time.time()
            print(f"hift time in ms: {(time_end - time_start) * 1000}")
        return tts_speech

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse
    import time
    
    parser = argparse.ArgumentParser(description="CosyVoiceCAæ¼”ç¤ºç¨‹åº")
    parser.add_argument("--model_dir", type=str, required=True, help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--output", type=str, default="output.wav", help="è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--mode", type=str, choices=['single', 'batch'], default='batch', help="æµ‹è¯•æ¨¡å¼: single=å•æ¡æµ‹è¯•, batch=æ‰¹é‡æµ‹è¯•")
    parser.add_argument("--text", type=str, default="ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·", help="å•æ¡æµ‹è¯•æ—¶ä½¿ç”¨çš„æ–‡æœ¬")
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    cosyvoice_ca = CosyVoiceCA(args.model_dir)
    all_tokens = []
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    import os
    os.makedirs("outputs", exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = open("log.txt", "w", encoding="utf-8")
    
    def log_and_print(message):
        """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        print(message)
        log_file.write(message + "\n")
        log_file.flush()
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©æµ‹è¯•æ–¹å¼
    if args.mode == 'single':
        # å•æ¡æµ‹è¯•æ¨¡å¼
        log_and_print(f"\n=== å•æ¡æµ‹è¯•æ¨¡å¼ ===")
        log_and_print(f"æµ‹è¯•æ–‡æœ¬: '{args.text}'")
        text_lines = [args.text]
    else:
        # ä»test.txtæ–‡ä»¶è¯»å–æµ‹è¯•æ–‡æœ¬ - æ‰¹é‡æµ‹è¯•æ¨¡å¼
        log_and_print(f"\n=== æ‰¹é‡æµ‹è¯•ï¼šä»test.txtæ–‡ä»¶è¯»å–æ‰€æœ‰æ–‡æœ¬ ===")
        txt_file_path = "test.txt"
        try:
            with open(txt_file_path, 'r', encoding='utf-8') as f:
                text_lines = [line.strip() for line in f.readlines() if line.strip()]
        except FileNotFoundError:
            log_and_print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {txt_file_path}ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬")
            text_lines = ["ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºå»èµ°èµ°ã€‚"]
        except Exception as e:
            log_and_print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬")
            text_lines = ["ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºå»èµ°èµ°ã€‚"]
        
        if not text_lines:
            log_and_print(f"æ–‡ä»¶ {txt_file_path} ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•æ–‡æœ¬")
            text_lines = ["ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºå»èµ°èµ°ã€‚"]
    
    log_and_print(f"å¼€å§‹æ‰¹é‡æµ‹è¯•ï¼Œå…±æ‰¾åˆ° {len(text_lines)} æ¡æ–‡æœ¬")
    if len(text_lines) <= 5:
        for i, text in enumerate(text_lines, 1):
            display_text = text[:50] + "..." if len(text) > 50 else text
            log_and_print(f"  ç¬¬{i}æ¡: '{display_text}'")
    else:
        log_and_print(f"  å‰3æ¡:")
        for i in range(3):
            display_text = text_lines[i][:50] + "..." if len(text_lines[i]) > 50 else text_lines[i]
            log_and_print(f"    ç¬¬{i+1}æ¡: '{display_text}'")
        log_and_print(f"  ... å…±{len(text_lines)}æ¡æ–‡æœ¬")
    
    # å­˜å‚¨æ€§èƒ½æ•°æ®
    first_packet_times = []
    rtf_values = []
    successful_count = 0
    last_audio = None
    
    for i, text in enumerate(text_lines, 1):
        if len(text) > 50:  # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œæ˜¾ç¤ºçœç•¥ç‰ˆæœ¬
            display_text = text[:50] + "..."
        else:
            display_text = text
        log_and_print(f"\n=== ç¬¬ {i}/{len(text_lines)} å¥: '{display_text}' ===")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        first_packet_time = None
        
        try:
            current_line_segments = []
            chunk_count = 0
            log_and_print("å¼€å§‹åˆæˆï¼Œå®æ—¶è¾“å‡ºéŸ³é¢‘å—...")
            
            for result in cosyvoice_ca.synthesize(text):
                chunk_count += 1
                # è®°å½•é¦–åŒ…æ—¶é—´
                if first_packet_time is None:
                    first_packet_time = time.time()
                    current_first_packet_delay = first_packet_time - start_time
                    log_and_print(f"ğŸ“¦ é¦–åŒ…æ—¶å»¶: {current_first_packet_delay:.2f}ç§’")
                    first_packet_times.append(current_first_packet_delay)
                
                current_line_segments.append(result['tts_speech'])
                current_audio_length = result['tts_speech'].shape[1] / cosyvoice_ca.sample_rate
                log_and_print(f"  ğŸ’« ç¬¬{chunk_count}ä¸ªéŸ³é¢‘å—: {current_audio_length:.2f}ç§’")
            
            # è®°å½•åˆæˆç»“æŸæ—¶é—´
            synthesis_time = time.time()
            synthesis_duration = synthesis_time - start_time
            log_and_print(f"â±ï¸ æ€»åˆæˆè€—æ—¶: {synthesis_duration:.2f}ç§’")
            log_and_print(f"ğŸ“Š æ€»å…±ç”Ÿæˆäº† {chunk_count} ä¸ªéŸ³é¢‘å—")
            
            # åˆå¹¶å½“å‰è¡Œçš„æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µå¹¶ä¿å­˜
            if current_line_segments:
                combined_line_audio = torch.cat(current_line_segments, dim=1)
                audio_length = combined_line_audio.shape[1] / cosyvoice_ca.sample_rate
                
                # è®¡ç®—å®æ—¶ç‡ (RTF = åˆæˆæ—¶é—´ / éŸ³é¢‘é•¿åº¦)
                rtf = synthesis_duration / audio_length
                rtf_values.append(rtf)
                log_and_print(f"ğŸš€ å®æ—¶ç‡(RTF): {rtf:.2f}")
                log_and_print(f"ğŸµ éŸ³é¢‘æ€»é•¿åº¦: {audio_length:.2f}ç§’")
                
                if args.mode == 'single':
                    line_output_path = f"outputs/single_test.wav"
                else:
                    line_output_path = f"outputs/batch_test_{i:04d}.wav"
                torchaudio.save(line_output_path, combined_line_audio, cosyvoice_ca.sample_rate)
                log_and_print(f"âœ… å·²ä¿å­˜éŸ³é¢‘: {line_output_path}")
                successful_count += 1
                
                # è®°å½•æœ€åä¸€å¥çš„éŸ³é¢‘ç”¨äºæœ€ç»ˆæŠ¥å‘Š
                if i == len(text_lines):
                    last_audio = combined_line_audio
            else:
                log_and_print(f"âŒ æ²¡æœ‰ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ")
                
        except Exception as e:
            log_and_print(f"âŒ ç¬¬ {i} è¡Œåˆæˆå¤±è´¥: {e}")
            import traceback
            log_and_print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            continue
    
    # ç»Ÿè®¡ç»“æœ
    log_and_print("\n" + "="*50)
    if args.mode == 'single':
        log_and_print("ğŸ“Š å•æ¡æµ‹è¯•ç»Ÿè®¡æŠ¥å‘Š")
    else:
        log_and_print("ğŸ“Š æ‰¹é‡æµ‹è¯•ç»Ÿè®¡æŠ¥å‘Š")
    log_and_print("="*50)
    
    log_and_print(f"æ€»æ–‡æœ¬æ•°é‡: {len(text_lines)}")
    log_and_print(f"æˆåŠŸåˆæˆ: {successful_count} å¥")
    log_and_print(f"å¤±è´¥åˆæˆ: {len(text_lines) - successful_count} å¥")
    log_and_print(f"æˆåŠŸç‡: {(successful_count/len(text_lines)*100):.1f}%")
    
    if first_packet_times and rtf_values:
        avg_first_packet = sum(first_packet_times) / len(first_packet_times)
        avg_rtf = sum(rtf_values) / len(rtf_values)
        
        log_and_print("\nâ±ï¸ é¦–åŒ…æ—¶å»¶ç»Ÿè®¡:")
        for i, delay in enumerate(first_packet_times[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            log_and_print(f"  ç¬¬{i}å¥: {delay:.2f}ç§’")
        if len(first_packet_times) > 10:
            log_and_print(f"  ... (å…±{len(first_packet_times)}å¥)")
        log_and_print(f"  å¹³å‡é¦–åŒ…æ—¶å»¶: {avg_first_packet:.2f}ç§’")
        
        log_and_print("\nğŸš€ RTFç»Ÿè®¡:")
        for i, rtf in enumerate(rtf_values[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            log_and_print(f"  ç¬¬{i}å¥: {rtf:.2f}")
        if len(rtf_values) > 10:
            log_and_print(f"  ... (å…±{len(rtf_values)}å¥)")
        log_and_print(f"  å¹³å‡RTF: {avg_rtf:.2f}")
        
        log_and_print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        log_and_print(f"  æœ€å¿«é¦–åŒ…æ—¶å»¶: {min(first_packet_times):.2f}ç§’")
        log_and_print(f"  æœ€æ…¢é¦–åŒ…æ—¶å»¶: {max(first_packet_times):.2f}ç§’")
        log_and_print(f"  æœ€ä½³RTF: {min(rtf_values):.2f}")
        log_and_print(f"  æœ€å·®RTF: {max(rtf_values):.2f}")
        
        # æ€»ç»“ä¿å­˜çš„éŸ³é¢‘æ–‡ä»¶
        log_and_print(f"\nğŸ’¾ å…±ä¿å­˜äº† {successful_count} ä¸ªéŸ³é¢‘æ–‡ä»¶:")
        if args.mode == 'single':
            log_and_print(f"  éŸ³é¢‘æ–‡ä»¶ä¿å­˜åœ¨: outputs/single_test.wav")
        else:
            log_and_print(f"  éŸ³é¢‘æ–‡ä»¶ä¿å­˜åœ¨: outputs/batch_test_XXXX.wav")
        
        # åŒæ—¶ä¿å­˜æœ€åä¸€å¥åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶
        if last_audio is not None:
            # å¦‚æœç”¨æˆ·æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶æ²¡æœ‰è·¯å¾„ï¼Œä¹Ÿæ”¾åˆ°outputsæ–‡ä»¶å¤¹
            if not os.path.dirname(args.output):
                final_output_path = f"outputs/{args.output}"
            else:
                final_output_path = args.output
            torchaudio.save(final_output_path, last_audio, cosyvoice_ca.sample_rate)
            final_audio_length = last_audio.shape[1] / cosyvoice_ca.sample_rate
            log_and_print(f"\nğŸ“ æœ€åä¸€å¥ä¹Ÿä¿å­˜ä¸º: {final_output_path}, é•¿åº¦: {final_audio_length:.2f}ç§’")
        
        log_and_print("="*50)
    else:
        log_and_print("\nâš ï¸ æ²¡æœ‰æˆåŠŸçš„åˆæˆè®°å½•ï¼Œæ— æ³•ç”Ÿæˆæ€§èƒ½ç»Ÿè®¡")
        log_and_print("="*50)
    
    # æœ€åçš„æ—¥å¿—ä¿¡æ¯
    log_and_print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: log.txt")
    
    # å…³é—­æ—¥å¿—æ–‡ä»¶
    log_file.close()