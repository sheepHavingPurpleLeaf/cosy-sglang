#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import numpy as np
import logging
from typing import List, Dict, Optional, Generator
from pathlib import Path
import threading
from contextlib import nullcontext
import os
import sys
sys.path.append('third_party/Matcha-TTS')
# æ·»åŠ yamlé…ç½®æ–‡ä»¶åŠ è½½
from hyperpyyaml import load_hyperpyyaml
from cosyvoice.utils.common import fade_in_out
# å¯¼å…¥æ‰€éœ€æ¨¡å‹ç±»
import tensorrt as trt
# å¯¼å…¥è¯·æ±‚åº“
import requests
import json
import time
import pdb
import torchaudio
import torch.nn.functional as F
from cosyvoice.utils.mask import make_pad_mask
from collections import deque
import queue

class CosyVoiceCA:
    """
    ä¸€ä¸ªç®€åŒ–ç‰ˆçš„CosyVoice2ç±»ï¼Œä»…åŠ è½½Flowå’ŒHiFiGANæ¨¡å‹ï¼Œ
    åªæ‰§è¡Œtoken2wavçš„æµå¼é€»è¾‘ï¼Œä¸æ‰§è¡ŒLLMéƒ¨åˆ†
    """
    
    def __init__(self, model_dir):
        """
        åˆå§‹åŒ–CosyVoiceCAç±»ï¼Œä½¿ç”¨å›ºå®šé…ç½®ï¼š
        load_jit=False, load_trt=True, fp16=True, use_flow_cache=True
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•ï¼ŒåŒ…å«flowå’Œhiftæ¨¡å‹
            device: æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡ï¼Œé»˜è®¤ä¸ºNoneï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self.device = torch.device('cuda')
        self.model_dir = Path(model_dir)
        
        # å›ºå®šé…ç½®
        self.fp16 = True
        self.is_synthesizing = False
        
        # åŠ è½½yamlé…ç½®æ–‡ä»¶ï¼Œä¸åŸç‰ˆæ–¹å¼ä¸€è‡´
        hyper_yaml_path = os.path.join(self.model_dir, "cosyvoice2ca.yaml")
        if not os.path.exists(hyper_yaml_path):
            raise ValueError(f"{hyper_yaml_path} é…ç½®æ–‡ä»¶ä¸å­˜åœ¨!")
        
        with open(hyper_yaml_path, 'r') as f:
            self.configs = load_hyperpyyaml(f, overrides={'qwen_pretrain_path': os.path.join(self.model_dir, 'CosyVoice-BlankEN')})
            self.sgl_config = self.configs.get('sgl_config', {})
            self.base_url = self.sgl_config.get('base_url', 'http://localhost:30000')
            
        # ä½¿ç”¨dequeæ›¿ä»£listï¼Œæé«˜æ€§èƒ½
        self.speech_tokens = deque()
        self.offset = 151936

        self.token_hop_len = 25
        self.token_overlap_len = 20
        self.llm_end = False
        # ä»é…ç½®è·å–é‡‡æ ·ç‡å’Œå…¶ä»–å‚æ•°
        self.sample_rate = self.configs.get('sample_rate', 24000)
        self.token_mel_ratio = self.configs.get('token_mel_ratio', 2)
        self.token_frame_rate = self.configs.get('token_frame_rate', 25)
        
        # åˆå§‹åŒ–æ¨¡å‹
        logging.info(f"Loading models from {self.model_dir}...")
        self._init_models()
        
        # è®¾ç½®flowç¼“å­˜å¤§å°
        self.mel_cache_len = 8
        self.source_cache_len = int(self.mel_cache_len * 480)
        self.speech_window = np.hamming(2 * self.source_cache_len)
        
        # ä½¿ç”¨ä¸CosyVoice2ç›¸åŒçš„ä¸Šä¸‹æ–‡ç®¡ç†
        self.llm_context = torch.cuda.stream(torch.cuda.Stream(self.device)) if torch.cuda.is_available() else nullcontext()
        
        # å¤šçº¿ç¨‹ç›¸å…³ - ä½¿ç”¨äº‹ä»¶æ›¿ä»£è½®è¯¢
        self.lock = threading.Lock()
        self.token_ready_event = threading.Event()
        self.token_queue = queue.Queue()
        
        self.to_save_tokens = []
        
        # é¢„åˆ†é…å¸¸ç”¨çš„å¼ é‡
        self.zero_cache_source = torch.zeros(1, 1, 0, device=self.device)
        
        # é¢„åˆ†é…å¼ é‡ç¼“å†²åŒºï¼Œé¿å…é¢‘ç¹åˆ›å»º
        max_token_len = self.token_hop_len + self.flow.pre_lookahead_len
        self.token_buffer = torch.zeros(1, max_token_len, dtype=torch.int32, device=self.device)
        self.token_len_buffer = torch.zeros(1, dtype=torch.int32, device=self.device)
        
        # å¤ç”¨sessionä»¥å‡å°‘è¿æ¥å¼€é”€
        self.session = requests.Session()
        
        # warmup_text = ["ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå–ä¸€æ¯å’–å•¡ã€‚", "å‰æ–¹å…«ç™¾ç±³å‘å³å‰æ–¹å¹¶å…¥åŒé“ã€‚", "å·²ç»å¸®ä½ æ“ä½œäº†", "è½¦é—¨å·²ç»æ‰“å¼€äº†", "å·²å°†éŸ³é‡è°ƒå¤§åˆ°ç™¾åˆ†ä¹‹ä¸‰å", "å‰¯é©¾åº§æ¤…å·²è°ƒç›´ï¼Œåº§æ¤…åŠ çƒ­å·²æ‰“å¼€"]
        # for i, text in enumerate(warmup_text, 1):
        #     print(f"Warmup {i}/{len(warmup_text)}: {text}")
        #     # ç­‰å¾…ä¸Šä¸€å¥å®Œå…¨ç»“æŸ
        #     while self.is_synthesizing:
        #         time.sleep(0.01)
        #     # å®Œå…¨æ¶ˆè´¹ç”Ÿæˆå™¨ï¼Œç¡®ä¿è¿™å¥è¯å®Œå…¨åˆæˆå®Œæ¯•
        #     for _ in self.synthesize(text):
        #         pass
        #     print(f"Warmup {i} completed")
        #     torch.save(self.to_save_tokens, os.path.join(self.model_dir, "warmup_tokens.pt"))
        if os.path.exists(os.path.join(self.model_dir, "warmup_tokens.pt")):
            logging.info("warmup start")
            warmup_tokens = torch.load(os.path.join(self.model_dir, "warmup_tokens.pt"))
            if isinstance(warmup_tokens, list):
                warmup_tokens = torch.stack(warmup_tokens)
            warmup_pointer = 0
            is_first_chunk = True
            while warmup_pointer < len(warmup_tokens):
                if is_first_chunk:
                    self.first_chunk_params['token'] = warmup_tokens[warmup_pointer:warmup_pointer + 28].unsqueeze(0)
                    self.first_chunk_params['token_len'] = torch.tensor([28], dtype=torch.int32).to(self.device)
                    _ = self.token2wav(**self.first_chunk_params)
                    is_first_chunk = False
                else:
                    self.other_chunk_params['token'] = warmup_tokens[warmup_pointer:warmup_pointer + 28].unsqueeze(0)
                    self.other_chunk_params['token_len'] = torch.tensor([28], dtype=torch.int32).to(self.device)
                    this_tts_speech = self.token2wav(**self.other_chunk_params)
                warmup_pointer += 28
            self.refresh_flow_cache()
        else:
            logging.info("Warmup tokens not found, skipping warmup")
        # Convert list of tensors to a single tensor
        logging.info(f"CosyVoiceCAåˆå§‹åŒ–å®Œæˆ")
        
    def _init_models(self):
        self.flow = self.configs['flow']
        self.flow_decoder_required_cache_size = self.token_hop_len * self.flow.token_mel_ratio
        self.hift = self.configs['hift']
        self.hift_cache = None
        self.tokenizer = self.configs['get_tokenizer']()
        
        # Add allowed_special attribute for tokenizer
        self.allowed_special = self.configs['allowed_special']

        self.token_min_hop_len = 2 * self.flow.input_frame_rate
        self.token_max_hop_len = 4 * self.flow.input_frame_rate
        
        # åº”ç”¨FP16è®¾ç½®
        if self.fp16 and torch.cuda.is_available():
            self.flow.half()
            
        # åŠ è½½æ¨¡å‹å‚æ•°
        self._load_models()
            
    def _load_models(self):
        """æŒ‰ç…§åŸç‰ˆåŠ è½½æ¨¡å‹æƒé‡"""
        try:
            # ç¡®å®šåŠ è½½çš„æ¨¡å‹è·¯å¾„
            flow_path = os.path.join(self.model_dir, "flow.cache.pt")
                
            hift_path = os.path.join(self.model_dir, "hift.pt")
            # spk2info = os.path.join(self.model_dir, "spk2info-old-newcreate.pt")
            spk2info = os.path.join(self.model_dir, "spk2info-tongtong-short.pt")
            # spk2info = os.path.join(self.model_dir, "spk2info-tong.pt")
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(flow_path):
                raise ValueError(f"Flowæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {flow_path}")
            if not os.path.exists(hift_path):
                raise ValueError(f"HiFTæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {hift_path}")
            if not os.path.exists(spk2info):
                raise ValueError(f"spk2infoæ–‡ä»¶ä¸å­˜åœ¨: {spk2info}")
            
            self.spk2info = torch.load(spk2info, map_location=self.device)['my_zero_shot_spk']
            
            # åŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸åŸç‰ˆCosyVoice2ä¸€è‡´
            self.flow.load_state_dict(torch.load(flow_path, map_location=self.device), strict=True)
            self.flow.to(self.device).eval()
            # flow_encoder = torch.jit.load(os.path.join(self.model_dir, "flow.encoder.fp16.zip"), map_location=self.device)
            # self.flow.encoder = flow_encoder
            self.flow_cache = self.init_flow_cache()
            flow_embedding = F.normalize(self.spk2info['flow_embedding'].half().to(self.device), dim=1)
            self.spk2info['flow_embedding'] = self.flow.spk_embed_affine_layer(flow_embedding).to(self.device)
            self.spk2info['mask'] = (~make_pad_mask(torch.tensor([self.token_hop_len + self.flow.pre_lookahead_len + self.spk2info['flow_prompt_speech_token'].size(1)]))).unsqueeze(-1).to(self.device)
            self.first_chunk_params = {
                'prompt_token': self.spk2info['flow_prompt_speech_token'].to(self.device),
                'prompt_token_len': torch.tensor([self.spk2info['flow_prompt_speech_token'].size(1)], dtype=torch.int32).to(self.device),
                'prompt_feat': self.spk2info['prompt_speech_feat'].to(self.device),
                'prompt_feat_len': torch.tensor([self.spk2info['prompt_speech_feat'].size(1)], dtype=torch.int32).to(self.device),
                'embedding': self.spk2info['flow_embedding'].to(self.device),
                'finalize': False,
                'is_first_chunk': True,
                'mask': self.spk2info['mask'].to(self.device)
            }  
            
            self.zero_prompt_token = torch.zeros(1, 0, dtype=torch.int32, device=self.device)
            self.zero_prompt_feat = torch.zeros(1, 0, 80, device=self.device)
            self.other_chunk_params = {
                'prompt_token': self.zero_prompt_token.to(self.device),
                'prompt_token_len': torch.tensor([self.zero_prompt_token.size(1)], dtype=torch.int32).to(self.device),
                'prompt_feat': self.zero_prompt_feat.to(self.device),
                'prompt_feat_len': torch.tensor([self.zero_prompt_feat.size(1)], dtype=torch.int32).to(self.device),
                'embedding': self.spk2info['flow_embedding'].to(self.device),
                'finalize': False,
                'is_first_chunk': False,
            } 
            self.last_chunk_params = {
                'prompt_token': self.zero_prompt_token.to(self.device),
                'prompt_token_len': torch.tensor([self.zero_prompt_token.size(1)], dtype=torch.int32).to(self.device),
                'prompt_feat': self.zero_prompt_feat.to(self.device),
                'prompt_feat_len': torch.tensor([self.zero_prompt_feat.size(1)], dtype=torch.int32).to(self.device),
                'embedding': self.spk2info['flow_embedding'].to(self.device),
                'finalize': True,
                'is_first_chunk': False,
            }
            
            # å¤„ç†HiFTæ¨¡å‹ï¼ŒæŒ‰ç…§CosyVoice2å¤„ç†æ–¹å¼
            hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_path, map_location=self.device).items()}
            self.hift.load_state_dict(hift_state_dict, strict=True)
            self.hift.to(self.device).eval()
            
            self._load_trt_models()
            
            logging.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: token_mel_ratio={self.token_mel_ratio}, token_frame_rate={self.token_frame_rate}")
        except Exception as e:
            logging.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
            
    def _load_trt_models(self):
        """åŠ è½½TensorRTæ¨¡å‹ï¼Œå¦‚æœéœ€è¦"""
        try:
            # TensorRTå¼•æ“æ–‡ä»¶è·¯å¾„
            flow_decoder_estimator_model = os.path.join(
                self.model_dir, 
                f"flow.decoder.estimator.fp16.mygpu.plan"
                # f"flow.decoder.estimator.fp16.4090.plan"
                # f"flow.decoder.estimator.fp16.A10.plan"
                # f"flow.decoder.estimator.fp16.3070.plan"
            )
            
            if os.path.getsize(flow_decoder_estimator_model) == 0:
                raise ValueError(f"{flow_decoder_estimator_model}æ˜¯ç©ºæ–‡ä»¶ï¼Œè¯·åˆ é™¤åé‡æ–°å¯¼å‡º!")
                
            # åŠ è½½TensorRTå¼•æ“
            del self.flow.decoder.estimator
            with open(flow_decoder_estimator_model, 'rb') as f:
                self.flow.decoder.estimator_engine = trt.Runtime(trt.Logger(trt.Logger.INFO)).deserialize_cuda_engine(f.read())
                
            assert self.flow.decoder.estimator_engine is not None, f"åŠ è½½TensorRTå¤±è´¥: {flow_decoder_estimator_model}"
            self.flow.decoder.estimator = self.flow.decoder.estimator_engine.create_execution_context()
            
            logging.info("TensorRTæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logging.error(f"TensorRTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def init_flow_cache(self):
        print(f"self.flow_decoder_required_cache_size: {self.flow_decoder_required_cache_size}")
        encoder_cache = {'offset': 0,
                         'pre_lookahead_layer_conv2_cache': torch.zeros(1, 512, 2).to(self.device),
                         'encoders_kv_cache': torch.zeros(6, 1, 8, 0, 64 * 2).to(self.device),
                         'upsample_offset': 0,
                         'upsample_conv_cache': torch.zeros(1, 512, 4).to(self.device),
                         'upsample_kv_cache': torch.zeros(4, 1, 8, 0, 64 * 2).to(self.device)}
        decoder_cache = {'offset': 0,
                         'down_blocks_conv_cache': torch.zeros(10, 1, 2, 832, 2).to(self.device),
                         'down_blocks_kv_cache': torch.zeros(10, 1, 4, 2, self.flow_decoder_required_cache_size, 512, 2).to(self.device),
                         'mid_blocks_conv_cache': torch.zeros(10, 12, 2, 512, 2).to(self.device),
                         'mid_blocks_kv_cache': torch.zeros(10, 12, 4, 2, self.flow_decoder_required_cache_size, 512, 2).to(self.device),
                         'up_blocks_conv_cache': torch.zeros(10, 1, 2, 1024, 2).to(self.device),
                         'up_blocks_kv_cache': torch.zeros(10, 1, 4, 2, self.flow_decoder_required_cache_size, 512, 2).to(self.device),
                         'final_blocks_conv_cache': torch.zeros(10, 2, 256, 2).to(self.device),
                         'down_blocks_kv_cache_out': torch.zeros(10, 1, 4, 2, 224, 512, 2).to(self.device),
                         'mid_blocks_kv_cache_out': torch.zeros(10, 12, 4, 2, 224, 512, 2).to(self.device),
                         'up_blocks_kv_cache_out': torch.zeros(10, 1, 4, 2, 224, 512, 2).to(self.device),
                         }
        if self.fp16 is True:
            for cache in [encoder_cache, decoder_cache]:
                for k, v in cache.items():
                    if isinstance(v, torch.Tensor):
                        cache[k] = v.half()
        cache = {'encoder_cache': encoder_cache, 'decoder_cache': decoder_cache}
        return cache

    def refresh_flow_cache(self):
        """é‡ç½®flow cacheï¼Œæ¢å¤åˆå§‹å½¢çŠ¶ä½†å¤ç”¨æ˜¾å­˜"""
        if hasattr(self, 'flow_cache') and self.flow_cache is not None:
            # é‡ç½®encoder cache
            encoder_cache = self.flow_cache['encoder_cache']
            encoder_cache['offset'] = 0
            dtype = torch.float16 if self.fp16 else torch.float32
            encoder_cache['pre_lookahead_layer_conv2_cache'] = torch.zeros(1, 512, 2, dtype=dtype, device=self.device)
            encoder_cache['encoders_kv_cache'] = torch.zeros(6, 1, 8, 0, 64 * 2, dtype=dtype, device=self.device)
            encoder_cache['upsample_offset'] = 0
            encoder_cache['upsample_conv_cache'] = torch.zeros(1, 512, 4, dtype=dtype, device=self.device)
            encoder_cache['upsample_kv_cache'] = torch.zeros(4, 1, 8, 0, 64 * 2, dtype=dtype, device=self.device)
            
            # é‡ç½®decoder cache  
            decoder_cache = self.flow_cache['decoder_cache']
            decoder_cache['offset'] = 0
            decoder_cache['down_blocks_conv_cache'] = torch.zero_(decoder_cache['down_blocks_conv_cache'])
            decoder_cache['down_blocks_kv_cache'] = torch.zero_(decoder_cache['down_blocks_kv_cache'])
            decoder_cache['mid_blocks_conv_cache'] = torch.zero_(decoder_cache['mid_blocks_conv_cache'])
            decoder_cache['mid_blocks_kv_cache'] = torch.zero_(decoder_cache['mid_blocks_kv_cache'])
            decoder_cache['up_blocks_conv_cache'] = torch.zero_(decoder_cache['up_blocks_conv_cache'])
            decoder_cache['up_blocks_kv_cache'] = torch.zero_(decoder_cache['up_blocks_kv_cache'])
            decoder_cache['final_blocks_conv_cache'] = torch.zero_(decoder_cache['final_blocks_conv_cache'])
            decoder_cache['down_blocks_kv_cache_out'] = torch.zero_(decoder_cache['down_blocks_kv_cache_out'])
            decoder_cache['mid_blocks_kv_cache_out'] = torch.zero_(decoder_cache['mid_blocks_kv_cache_out'])
            decoder_cache['up_blocks_kv_cache_out'] = torch.zero_(decoder_cache['up_blocks_kv_cache_out'])
        else:
            # å¦‚æœcacheä¸å­˜åœ¨ï¼Œåˆ™åˆå§‹åŒ–
            self.flow_cache = self.init_flow_cache()

    def _send_stream_request(self, payload):
        """å‘é€æµå¼è¯·æ±‚å¹¶å¤„ç†å“åº”æµ - ä¼˜åŒ–ç‰ˆæœ¬"""

        processed_tokens = 0
        try:
            # å‘é€è¯·æ±‚ - å¤ç”¨session
            response = self.session.post(
                self.base_url + "/generate",
                json=payload,
                stream=True,
                timeout=60
            )
            
            if response.status_code != 200:
                logging.error(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                yield {"error": f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"}
                return
            
            # å¤„ç†æµå¼å“åº” - å‡å°‘ä¸å¿…è¦çš„è®¡ç®—
            for chunk in response.iter_lines(decode_unicode=False):
                if not chunk:
                    continue
                chunk_str = chunk.decode("utf-8")
                
                if chunk_str == "data: [DONE]":
                    self.llm_end = True
                    break
                    
                if chunk_str.startswith("data:"):
                    try:
                        data = json.loads(chunk_str[5:].strip())
                        if "output_ids" in data:
                            current_tokens = data["output_ids"]
                            
                            # è®¡ç®—æ–°å¢çš„token
                            new_tokens = current_tokens[processed_tokens:]
                            processed_tokens = len(current_tokens)
                            
                            # æ·»åŠ åˆ°æ€»tokenåˆ—è¡¨
                            yield {"tokens": new_tokens, "is_partial": True}
                            
                    except json.JSONDecodeError as e:
                        logging.error(f"è§£æJSONæ—¶å‡ºé”™: {e} - åŸå§‹æ•°æ®: {chunk_str}")
            
                
        except requests.RequestException as e:
            logging.error(f"è¯·æ±‚é”™è¯¯: {str(e)}")
            yield {"error": f"è¯·æ±‚é”™è¯¯: {str(e)}"}
        except Exception as e:
            logging.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}")
            yield {"error": f"å¤„ç†å“åº”é”™è¯¯: {str(e)}"}
    
    def _prepare_input_features(self, text: str):
        """å‡†å¤‡è¾“å…¥ç‰¹å¾"""
        text_token = self.tokenizer.encode(text, allowed_special=self.allowed_special)
        # print(text_token, text)
        text_len = len(text_token)
        model_input = self.spk2info
        prompt_text = model_input['prompt_text'].squeeze().tolist()
        llm_input = [158500] + prompt_text +text_token + [158497]
        # text_token = torch.tensor([text_token], dtype=torch.int32).to(self.device)
        # text_token_len = torch.tensor([text_token.shape[1]], dtype=torch.int32).to(self.device)

        speech_prompt_token = model_input['llm_prompt_speech_token'] + self.offset
        llm_input = llm_input + speech_prompt_token.squeeze().tolist()
        model_input['llm_input'] = llm_input
        model_input['text_len'] = text_len
        return model_input


        # è·å–è¾“å…¥æ–‡æœ¬çš„token
    def _llm_job(self, input_ids: List[int], text_len: int):
        """
        å¤„ç†LLMä»»åŠ¡
        
        Args:
            input_ids: è¾“å…¥çš„token IDs
        """
        payload = {
            "input_ids": input_ids,
            "stream": True,
            "sampling_params": {
                "top_p": 0.8, 
                "top_k": 25, 
                "temperature": 1.0, 
                "max_new_tokens": text_len * self.sgl_config.get('max_token_text_ratio', 20), 
                "ras_penalty": 3.0,
                "stop_token_ids": [158497]
            }
        }
        for i in self._send_stream_request(payload):
            
            # ä½¿ç”¨é”ä¿æŠ¤å¯¹speech_tokensçš„ä¿®æ”¹
            with self.lock:
                self.speech_tokens.extend(i["tokens"])
                if self.speech_tokens and self.speech_tokens[-1] == 6561 + self.offset:
                    self.speech_tokens.pop()
            
            # if len(self.speech_tokens) >= self.token_hop_len + self.flow.pre_lookahead_len:
            self.token_ready_event.set()
        
        # LLMä»»åŠ¡ç»“æŸæ ‡å¿—
        with self.lock:
            self.llm_end = True
        self.token_ready_event.set()  # ç¡®ä¿ä¸»çº¿ç¨‹èƒ½è¢«å”¤é†’
    
    def synthesize(self, text: str, session_id: str = "default") -> Generator[Dict, None, None]:
        """
        åˆæˆè¯­éŸ³
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            session_id: ä¼šè¯ID
            
        Returns:
            ç”Ÿæˆå™¨ï¼Œäº§ç”ŸåŒ…å«ç”Ÿæˆçš„tokençš„å­—å…¸
        """
        if self.is_synthesizing is True:
            logging.info("æ­£åœ¨åˆæˆä¸­ï¼Œè¯·ç¨åå†è¯•")
            return
        
        self.is_synthesizing = True
        self.hift_cache = None  # é‡ç½®hiftç¼“å­˜
        
        # ä½¿ç”¨é”ä¿æŠ¤çŠ¶æ€åˆå§‹åŒ–
        with self.lock:
            self.speech_tokens = []  # é‡ç½®è¯­éŸ³token
            self.llm_end = False  # é‡ç½®LLMç»“æŸæ ‡å¿—
        
        model_input = self._prepare_input_features(text)
        is_first_chunk = True
        p = threading.Thread(target=self._llm_job, args=(model_input['llm_input'], model_input['text_len']))
        p.start()
        print("synthesize text:", text)
        while True:
            # Wait for tokens to be ready, but don't rely on timeout alone
            self.token_ready_event.wait(timeout=0.1)
            
            # ä½¿ç”¨é”ä¿æŠ¤å¯¹speech_tokensçš„è®¿é—®å’ŒçŠ¶æ€æ£€æŸ¥
            with self.lock:
                # Check if we have tokens available before attempting to pop
                if not self.speech_tokens:
                    # If no tokens available but LLM has ended, break the loop
                    if self.llm_end:
                        break
                    # Reset the event and continue waiting
                    self.token_ready_event.clear()
                    continue
                
                # å¤åˆ¶éœ€è¦çš„tokensï¼Œé¿å…é•¿æ—¶é—´æŒæœ‰é”
                tokens_needed = min(len(self.speech_tokens), 
                                   self.token_hop_len + self.flow.pre_lookahead_len)
                current_tokens = list(self.speech_tokens[:tokens_needed])
            
            # Clear the event after consuming a token
            self.token_ready_event.clear()
            this_tts_speech_token = torch.tensor(current_tokens, device=self.device).unsqueeze(dim=0) - self.offset
            # if len(this_tts_speech_token[0]) == 28:
            #     self.to_save_tokens.extend(this_tts_speech_token[0])
            
            if is_first_chunk:
                self.first_chunk_params['token'] = this_tts_speech_token
                self.first_chunk_params['token_len'] = torch.tensor([this_tts_speech_token.shape[1]], dtype=torch.int32).to(self.device)
                this_tts_speech = self.token2wav(**self.first_chunk_params)
                is_first_chunk = False
            else:
                self.other_chunk_params['token'] = this_tts_speech_token
                self.other_chunk_params['token_len'] = torch.tensor([this_tts_speech_token.shape[1]], dtype=torch.int32).to(self.device)
                this_tts_speech = self.token2wav(**self.other_chunk_params)
            yield {'tts_speech': this_tts_speech.cpu()}
            
            # ä½¿ç”¨é”ä¿æŠ¤ç§»é™¤æ“ä½œå’ŒçŠ¶æ€æ£€æŸ¥
            with self.lock:
                self.speech_tokens = self.speech_tokens[self.token_hop_len:]
                should_break = (self.llm_end is True and 
                               len(self.speech_tokens) < self.token_hop_len + self.flow.pre_lookahead_len)
            
            if should_break:
                break
        p.join()
        
        # ä½¿ç”¨é”ä¿æŠ¤æœ€åçš„tokenå¤„ç†
        with self.lock:
            remaining_tokens = list(self.speech_tokens) if self.speech_tokens else []
        
        if len(remaining_tokens) >= 4:
            this_tts_speech_token = torch.tensor(remaining_tokens).unsqueeze(dim=0) - self.offset
            self.last_chunk_params['token'] = this_tts_speech_token
            self.last_chunk_params['token_len'] = torch.tensor([this_tts_speech_token.shape[1]], dtype=torch.int32).to(self.device)
            this_tts_speech = self.token2wav(**self.last_chunk_params)
            
            # èˆå¼ƒæœ€å40msçš„éŸ³é¢‘ (40ms * sample_rate = æ ·æœ¬æ•°)
            samples_to_remove = int(0.04 * self.sample_rate)  # 40mså¯¹åº”çš„é‡‡æ ·ç‚¹æ•°
            if this_tts_speech.shape[1] > samples_to_remove:
                this_tts_speech = this_tts_speech[:, :-samples_to_remove]
            
            yield {'tts_speech': this_tts_speech.cpu()}
        
        #é‡ç½®flow cache
        self.refresh_flow_cache()
        
        # ä½¿ç”¨é”ä¿æŠ¤çŠ¶æ€é‡ç½®
        with self.lock:
            self.speech_tokens = []
            self.llm_end = False
        self.is_synthesizing = False
            
    def token2wav(self, token, token_len, prompt_token, prompt_token_len, prompt_feat, prompt_feat_len, embedding, finalize=False, is_first_chunk=False, mask=None):
        with torch.cuda.amp.autocast(self.fp16):
            flow_start = time.time()
            tts_mel, self.flow_cache = self.flow.inference(token=token.to(self.device),
                                                                      token_len=token_len,
                                                                      prompt_token=prompt_token,
                                                                      prompt_token_len=prompt_token_len,
                                                                      prompt_feat=prompt_feat,
                                                                      prompt_feat_len=prompt_feat_len,
                                                                      embedding=embedding,
                                                                      cache=self.flow_cache,
                                                                      is_first_chunk=is_first_chunk,
                                                                      finalize=finalize,
                                                                      mask=mask)
                # append hift cache
            flow_end = time.time()
            # print(f"flow inference time: {flow_end - flow_start:.2f}s")
            if self.hift_cache is not None:
                hift_cache_mel, hift_cache_source = self.hift_cache['mel'], self.hift_cache['source']
                tts_mel = torch.concat([hift_cache_mel, tts_mel], dim=2)
            else:
                hift_cache_source = self.zero_cache_source  # ä½¿ç”¨é¢„åˆ†é…çš„é›¶å¼ é‡
            # keep overlap mel and hift cache
            if finalize is False:
                tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)
                if self.hift_cache is not None:
                    tts_speech = fade_in_out(tts_speech, self.hift_cache['speech'], self.speech_window)
                self.hift_cache = {'mel': tts_mel[:, :, -self.mel_cache_len:],
                                            'source': tts_source[:, :, -self.source_cache_len:],
                                            'speech': tts_speech[:, -self.source_cache_len:]}
                tts_speech = tts_speech[:, :-self.source_cache_len]
            else:
                tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)
                if self.hift_cache is not None:
                    tts_speech = fade_in_out(tts_speech, self.hift_cache['speech'], self.speech_window)
            # print(f"hift inference time: {time.time() - flow_end:.2f}s")
        return tts_speech

    def _safe_cache_copy(self, cache_dict, deep_copy=False):
        """
        å®‰å…¨çš„cacheå¤åˆ¶æ–¹æ³•
        
        Args:
            cache_dict: è¦å¤åˆ¶çš„cacheå­—å…¸
            deep_copy: æ˜¯å¦è¿›è¡Œæ·±æ‹·è´
            
        Returns:
            å¤åˆ¶åçš„cacheå­—å…¸
        """
        if cache_dict is None:
            return None
            
        safe_cache = {}
        for key, value in cache_dict.items():
            if isinstance(value, torch.Tensor):
                if deep_copy:
                    # æ·±æ‹·è´ï¼šå®Œå…¨ç‹¬ç«‹çš„å†…å­˜
                    safe_cache[key] = value.clone().detach()
                else:
                    # æµ…æ‹·è´ï¼šå…±äº«æ•°æ®ä½†ç‹¬ç«‹çš„å¼ é‡å¯¹è±¡
                    safe_cache[key] = value.detach()
            else:
                # éå¼ é‡å€¼ç›´æ¥å¤åˆ¶
                safe_cache[key] = value
        return safe_cache
    
    def _validate_cache_memory_safety(self, cache_dict):
        """
        éªŒè¯cacheçš„å†…å­˜å®‰å…¨æ€§
        
        Args:
            cache_dict: è¦éªŒè¯çš„cacheå­—å…¸
            
        Returns:
            bool: æ˜¯å¦é€šè¿‡éªŒè¯
        """
        if cache_dict is None:
            return False
            
        try:
            for key, tensor in cache_dict.items():
                if isinstance(tensor, torch.Tensor):
                    # æ£€æŸ¥å¼ é‡æ˜¯å¦è¿ç»­
                    if not tensor.is_contiguous():
                        print(f"è­¦å‘Š: cache[{key}] ä¸æ˜¯è¿ç»­å†…å­˜å¸ƒå±€")
                        return False
                    
                    # æ£€æŸ¥å†…å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆé€šè¿‡è®¿é—®data_ptrï¼‰
                    _ = tensor.data_ptr()
                    
                    # æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
                    if tensor.device != self.device:
                        print(f"è­¦å‘Š: cache[{key}] è®¾å¤‡ä¸åŒ¹é…ï¼ŒæœŸæœ›{self.device}ï¼Œå®é™…{tensor.device}")
                        return False
                        
            return True
        except Exception as e:
            print(f"cacheå†…å­˜éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _safe_forward_chunk_call(self, encoder, token, token_len, context=None, cache=None, use_safe_copy=True):
        """
        å®‰å…¨çš„forward_chunkè°ƒç”¨
        
        Args:
            encoder: encoderå¯¹è±¡
            token: è¾“å…¥token
            token_len: tokené•¿åº¦
            context: ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            cache: cacheå­—å…¸
            use_safe_copy: æ˜¯å¦ä½¿ç”¨å®‰å…¨å¤åˆ¶
            
        Returns:
            tuple: (h, h_lengths, new_cache)
        """
        if cache is None:
            cache = {}
            
        # éªŒè¯è¾“å…¥cacheçš„å®‰å…¨æ€§
        if not self._validate_cache_memory_safety(cache):
            print("è¾“å…¥cacheéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤cache")
            cache = {}
        
        # å‡†å¤‡å®‰å…¨çš„cacheå‚æ•°
        if use_safe_copy and cache:
            safe_cache = self._safe_cache_copy(cache, deep_copy=False)
        else:
            safe_cache = cache
            
        try:
            # è°ƒç”¨forward_chunk
            if context is not None:
                result = encoder.forward_chunk(token, token_len, context=context, **safe_cache)
            else:
                result = encoder.forward_chunk(token, token_len, **safe_cache)
                
            # éªŒè¯è¾“å‡º
            h, h_lengths, new_cache = result
            
            # æ£€æŸ¥è¾“å‡ºçš„æœ‰æ•ˆæ€§
            if torch.isnan(h).any() or torch.isinf(h).any():
                raise RuntimeError("forward_chunkè¾“å‡ºåŒ…å«NaNæˆ–Infå€¼")
                
            return h, h_lengths, new_cache
            
        except RuntimeError as e:
            if "CUDA error" in str(e) or "device-side assert" in str(e):
                print(f"CUDAé”™è¯¯ï¼Œå¯èƒ½ä¸cacheå†…å­˜çŠ¶æ€æœ‰å…³: {e}")
                # æ¸…ç†å¯èƒ½æŸåçš„å†…å­˜çŠ¶æ€
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
            raise e

    def demonstrate_cache_passing_risks(self):
        """
        æ¼”ç¤ºä¸åŒcacheä¼ é€’æ–¹å¼çš„é£é™©å’Œå¯¹æ¯”
        
        è¿™ä¸ªæ–¹æ³•å±•ç¤ºäº†ï¼š
        1. ç›´æ¥ä¼ é€’çš„é£é™©
        2. ä½¿ç”¨data_ptr()çš„åœºæ™¯
        3. å®‰å…¨ä¼ é€’çš„æœ€ä½³å®è·µ
        """
        print("=== Cacheä¼ é€’æ–¹å¼é£é™©åˆ†ææ¼”ç¤º ===")
        
        # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹cache
        test_cache = {
            'offset': 0,
            'conv_cache': torch.randn(1, 512, 2, device=self.device),
            'kv_cache': torch.randn(6, 1, 8, 0, 128, device=self.device)
        }
        
        print("1. åŸå§‹cacheçŠ¶æ€:")
        for key, value in test_cache.items():
            if isinstance(value, torch.Tensor):
                print(f"   {key}: shape={value.shape}, data_ptr={hex(value.data_ptr())}")
        
        # æ–¹å¼1: ç›´æ¥ä¼ é€’å¼•ç”¨ï¼ˆé£é™©è¾ƒé«˜ï¼‰
        print("\n2. ç›´æ¥ä¼ é€’å¼•ç”¨çš„é£é™©:")
        def risky_function(**cache_params):
            """æ¨¡æ‹Ÿä¸€ä¸ªå¯èƒ½ä¿®æ”¹cacheçš„å‡½æ•°"""
            if 'conv_cache' in cache_params:
                # ç›´æ¥ä¿®æ”¹ä¼ å…¥çš„å¼ é‡
                cache_params['conv_cache'].fill_(999.0)  # é£é™©æ“ä½œ
                print(f"   å‡½æ•°å†…ä¿®æ”¹å conv_cache å‰å‡ ä¸ªå€¼: {cache_params['conv_cache'].flatten()[:3]}")
        
        print("   è°ƒç”¨å‰ conv_cache å‰å‡ ä¸ªå€¼:", test_cache['conv_cache'].flatten()[:3])
        risky_function(**test_cache)  # ç›´æ¥ä¼ é€’
        print("   è°ƒç”¨å conv_cache å‰å‡ ä¸ªå€¼:", test_cache['conv_cache'].flatten()[:3])
        print("   âŒ åŸå§‹cacheè¢«æ„å¤–ä¿®æ”¹ï¼")
        
        # æ–¹å¼2: ä½¿ç”¨data_ptr()ï¼ˆåº•å±‚æ“ä½œï¼‰
        print("\n3. data_ptr()æ–¹å¼çš„ç‰¹ç‚¹:")
        conv_cache = test_cache['conv_cache']
        print(f"   data_ptr: {hex(conv_cache.data_ptr())}")
        print(f"   is_contiguous: {conv_cache.is_contiguous()}")
        print(f"   element_size: {conv_cache.element_size()} bytes")
        print(f"   æ€»å†…å­˜å¤§å°: {conv_cache.numel() * conv_cache.element_size()} bytes")
        
        # æ£€æŸ¥å†…å­˜è¿ç»­æ€§çš„é‡è¦æ€§
        non_contiguous = conv_cache.transpose(0, 1)  # åˆ›å»ºéè¿ç»­å¼ é‡
        print(f"   è½¬ç½®å is_contiguous: {non_contiguous.is_contiguous()}")
        print("   ğŸ’¡ éè¿ç»­å¼ é‡éœ€è¦è°ƒç”¨ .contiguous() æ‰èƒ½å®‰å…¨ä½¿ç”¨data_ptr()")
        
        # æ–¹å¼3: å®‰å…¨å¤åˆ¶ï¼ˆæ¨èï¼‰
        print("\n4. å®‰å…¨å¤åˆ¶æ–¹å¼:")
        safe_cache = self._safe_cache_copy(test_cache, deep_copy=False)
        
        def safe_function(**cache_params):
            if 'conv_cache' in cache_params:
                cache_params['conv_cache'].fill_(777.0)
                print(f"   å®‰å…¨å‡½æ•°å†…ä¿®æ”¹å: {cache_params['conv_cache'].flatten()[:3]}")
        
        print("   åŸå§‹cache:", test_cache['conv_cache'].flatten()[:3])
        safe_function(**safe_cache)
        print("   è°ƒç”¨ååŸå§‹cache:", test_cache['conv_cache'].flatten()[:3])
        print("   âœ… åŸå§‹cacheæœªè¢«ä¿®æ”¹ï¼Œä½†å…±äº«åº•å±‚å†…å­˜ï¼ˆé€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼‰")
        
        # æ–¹å¼4: æ·±æ‹·è´ï¼ˆæœ€å®‰å…¨ä½†å¼€é”€å¤§ï¼‰
        print("\n5. æ·±æ‹·è´æ–¹å¼:")
        deep_cache = self._safe_cache_copy(test_cache, deep_copy=True)
        print(f"   åŸå§‹ data_ptr: {hex(test_cache['conv_cache'].data_ptr())}")
        print(f"   æ·±æ‹·è´ data_ptr: {hex(deep_cache['conv_cache'].data_ptr())}")
        print("   âœ… å®Œå…¨ç‹¬ç«‹çš„å†…å­˜ï¼Œä½†æ¶ˆè€—æ›´å¤šæ˜¾å­˜")
        
        print("\n=== æ¨èä½¿ç”¨åœºæ™¯ ===")
        print("1. æ™®é€šæ¨ç†: ä½¿ç”¨æµ…æ‹·è´ (_safe_cache_copy(deep_copy=False))")
        print("2. æ‰¹é‡æµ‹è¯•: ä½¿ç”¨æ·±æ‹·è´ (_safe_cache_copy(deep_copy=True))")
        print("3. TensorRT/C++è°ƒç”¨: ä½¿ç”¨ tensor.contiguous().data_ptr()")
        print("4. è°ƒè¯•é—®é¢˜: ä½¿ç”¨ _validate_cache_memory_safety() æ£€æŸ¥")
    
    def get_cache_memory_info(self, cache_dict, name="cache"):
        """
        è·å–cacheçš„è¯¦ç»†å†…å­˜ä¿¡æ¯
        
        Args:
            cache_dict: cacheå­—å…¸
            name: cacheåç§°
            
        Returns:
            str: å†…å­˜ä¿¡æ¯æ‘˜è¦
        """
        if cache_dict is None:
            return f"{name}: None"
        
        info_lines = [f"{name} å†…å­˜ä¿¡æ¯:"]
        total_memory = 0
        
        for key, value in cache_dict.items():
            if isinstance(value, torch.Tensor):
                memory_size = value.numel() * value.element_size()
                total_memory += memory_size
                info_lines.append(
                    f"  {key}: {value.shape} | {memory_size/1024/1024:.2f}MB | "
                    f"ptr={hex(value.data_ptr())} | contiguous={value.is_contiguous()}"
                )
            else:
                info_lines.append(f"  {key}: {value} (éå¼ é‡)")
        
        info_lines.append(f"æ€»å†…å­˜ä½¿ç”¨: {total_memory/1024/1024:.2f}MB")
        return "\n".join(info_lines)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse
    import time
    
    parser = argparse.ArgumentParser(description="CosyVoiceCAæ¼”ç¤ºç¨‹åº")
    parser.add_argument("--model_dir", type=str, required=True, help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--output", type=str, default="output.wav", help="è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    cosyvoice_ca = CosyVoiceCA(args.model_dir)
    all_tokens = []
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    import os
    os.makedirs("outputs", exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = open("log.txt", "w", encoding="utf-8")
    
    def log_and_print(message):
        """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        print(message)
        log_file.write(message + "\n")
        log_file.flush()
    
    # ä»test.txtæ–‡ä»¶è¯»å–æµ‹è¯•æ–‡æœ¬
    log_and_print(f"\n=== æ‰¹é‡æµ‹è¯•ï¼šä»test.txtæ–‡ä»¶è¯»å–æ–‡æœ¬ ===")
    txt_file_path = "test.txt"
    try:
        with open(txt_file_path, 'r', encoding='utf-8') as f:
            text_lines = [line.strip() for line in f.readlines() if line.strip()]
    except FileNotFoundError:
        log_and_print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {txt_file_path}ï¼Œé€€å‡ºç¨‹åº")
        log_file.close()
        exit(1)
    except Exception as e:
        log_and_print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œé€€å‡ºç¨‹åº")
        log_file.close()
        exit(1)
    
    if not text_lines:
        log_and_print(f"æ–‡ä»¶ {txt_file_path} ä¸ºç©ºï¼Œé€€å‡ºç¨‹åº")
        log_file.close()
        exit(1)
        
    log_and_print(f"å¼€å§‹ä» {txt_file_path} é€è¡Œåˆæˆæ–‡æœ¬...")
    log_and_print(f"å…±æ‰¾åˆ° {len(text_lines)} è¡Œæ–‡æœ¬")
    
    # å­˜å‚¨æ€§èƒ½æ•°æ®
    first_packet_times = []
    rtf_values = []
    successful_count = 0
    last_audio = None
    
    for i, text in enumerate(text_lines, 1):
        if len(text) > 50:  # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œæ˜¾ç¤ºçœç•¥ç‰ˆæœ¬
            display_text = text[:50] + "..."
        else:
            display_text = text
        log_and_print(f"\n=== ç¬¬ {i}/{len(text_lines)} å¥: '{display_text}' ===")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        first_packet_time = None
        
        try:
            current_line_segments = []
            for result in cosyvoice_ca.synthesize(text):
                # è®°å½•é¦–åŒ…æ—¶é—´
                if first_packet_time is None:
                    first_packet_time = time.time()
                    current_first_packet_delay = first_packet_time - start_time
                    log_and_print(f"é¦–åŒ…æ—¶å»¶: {current_first_packet_delay:.2f}ç§’")
                    first_packet_times.append(current_first_packet_delay)
                    
                current_line_segments.append(result['tts_speech'])
            
            # è®°å½•åˆæˆç»“æŸæ—¶é—´
            synthesis_time = time.time()
            synthesis_duration = synthesis_time - start_time
            log_and_print(f"åˆæˆè€—æ—¶: {synthesis_duration:.2f}ç§’")
            
            # åˆå¹¶å½“å‰è¡Œçš„æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µå¹¶ä¿å­˜
            if current_line_segments:
                combined_line_audio = torch.cat(current_line_segments, dim=1)
                audio_length = combined_line_audio.shape[1] / cosyvoice_ca.sample_rate
                
                # è®¡ç®—å®æ—¶ç‡ (RTF = åˆæˆæ—¶é—´ / éŸ³é¢‘é•¿åº¦)
                rtf = synthesis_duration / audio_length
                rtf_values.append(rtf)
                log_and_print(f"å®æ—¶ç‡(RTF): {rtf:.2f}")
                log_and_print(f"éŸ³é¢‘é•¿åº¦: {audio_length:.2f}ç§’")
                
                line_output_path = f"outputs/batch_test_{i:04d}.wav"
                torchaudio.save(line_output_path, combined_line_audio, cosyvoice_ca.sample_rate)
                log_and_print(f"âœ“ å·²ä¿å­˜éŸ³é¢‘: {line_output_path}")
                successful_count += 1
                
                # è®°å½•æœ€åä¸€å¥çš„éŸ³é¢‘ç”¨äºæœ€ç»ˆæŠ¥å‘Š
                if i == len(text_lines):
                    last_audio = combined_line_audio
            else:
                log_and_print(f"âŒ ç¬¬ {i} è¡Œæ²¡æœ‰ç”ŸæˆéŸ³é¢‘ç‰‡æ®µ")
                
        except Exception as e:
            log_and_print(f"ç¬¬ {i} è¡Œåˆæˆå¤±è´¥: {e}")
            continue
    
    # ç»Ÿè®¡ç»“æœ
    log_and_print("\n" + "="*50)
    log_and_print("ğŸ“Š æ‰¹é‡æµ‹è¯•ç»Ÿè®¡æŠ¥å‘Š")
    log_and_print("="*50)
    
    log_and_print(f"æ€»æ–‡æœ¬æ•°é‡: {len(text_lines)}")
    log_and_print(f"æˆåŠŸåˆæˆ: {successful_count} å¥")
    log_and_print(f"å¤±è´¥åˆæˆ: {len(text_lines) - successful_count} å¥")
    log_and_print(f"æˆåŠŸç‡: {(successful_count/len(text_lines)*100):.1f}%")
    
    if first_packet_times and rtf_values:
        avg_first_packet = sum(first_packet_times) / len(first_packet_times)
        avg_rtf = sum(rtf_values) / len(rtf_values)
        
        log_and_print("\nâ±ï¸ é¦–åŒ…æ—¶å»¶ç»Ÿè®¡:")
        for i, delay in enumerate(first_packet_times[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            log_and_print(f"  ç¬¬{i}å¥: {delay:.2f}ç§’")
        if len(first_packet_times) > 10:
            log_and_print(f"  ... (å…±{len(first_packet_times)}å¥)")
        log_and_print(f"  å¹³å‡é¦–åŒ…æ—¶å»¶: {avg_first_packet:.2f}ç§’")
        
        log_and_print("\nğŸš€ RTFç»Ÿè®¡:")
        for i, rtf in enumerate(rtf_values[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            log_and_print(f"  ç¬¬{i}å¥: {rtf:.2f}")
        if len(rtf_values) > 10:
            log_and_print(f"  ... (å…±{len(rtf_values)}å¥)")
        log_and_print(f"  å¹³å‡RTF: {avg_rtf:.2f}")
        
        log_and_print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        log_and_print(f"  æœ€å¿«é¦–åŒ…æ—¶å»¶: {min(first_packet_times):.2f}ç§’")
        log_and_print(f"  æœ€æ…¢é¦–åŒ…æ—¶å»¶: {max(first_packet_times):.2f}ç§’")
        log_and_print(f"  æœ€ä½³RTF: {min(rtf_values):.2f}")
        log_and_print(f"  æœ€å·®RTF: {max(rtf_values):.2f}")
        
        # æ€»ç»“ä¿å­˜çš„éŸ³é¢‘æ–‡ä»¶
        log_and_print(f"\nğŸ’¾ å…±ä¿å­˜äº† {successful_count} ä¸ªéŸ³é¢‘æ–‡ä»¶:")
        log_and_print(f"  éŸ³é¢‘æ–‡ä»¶ä¿å­˜åœ¨: outputs/batch_test_XXXX.wav")
        
        # åŒæ—¶ä¿å­˜æœ€åä¸€å¥åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶
        if last_audio is not None:
            # å¦‚æœç”¨æˆ·æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶æ²¡æœ‰è·¯å¾„ï¼Œä¹Ÿæ”¾åˆ°outputsæ–‡ä»¶å¤¹
            if not os.path.dirname(args.output):
                final_output_path = f"outputs/{args.output}"
            else:
                final_output_path = args.output
            torchaudio.save(final_output_path, last_audio, cosyvoice_ca.sample_rate)
            final_audio_length = last_audio.shape[1] / cosyvoice_ca.sample_rate
            log_and_print(f"\nğŸ“ æœ€åä¸€å¥ä¹Ÿä¿å­˜ä¸º: {final_output_path}, é•¿åº¦: {final_audio_length:.2f}ç§’")
        
        log_and_print("="*50)
    else:
        log_and_print("\nâš ï¸ æ²¡æœ‰æˆåŠŸçš„åˆæˆè®°å½•ï¼Œæ— æ³•ç”Ÿæˆæ€§èƒ½ç»Ÿè®¡")
        log_and_print("="*50)
    
    # æœ€åçš„æ—¥å¿—ä¿¡æ¯
    log_and_print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: log.txt")
    
    # å…³é—­æ—¥å¿—æ–‡ä»¶
    log_file.close()
